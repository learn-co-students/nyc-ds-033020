{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated when set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=10000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=0.38245219347581555,\n",
       "              seed=None, silent=None, subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793722\n",
      "F1: 0.676056\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.639739</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.616502</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599252</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>0.038028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.434456</td>\n",
       "      <td>0.038454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.371195</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.370798</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.038613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.370584</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.038632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.639739           0.006761           0.640184   \n",
       "2              0.627790           0.006070           0.629019   \n",
       "3              0.614868           0.010608           0.616502   \n",
       "4              0.599252           0.010671           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "127            0.372616           0.009112           0.434313   \n",
       "128            0.371920           0.009004           0.434456   \n",
       "129            0.371195           0.009250           0.434110   \n",
       "130            0.370798           0.009382           0.434108   \n",
       "131            0.370584           0.009246           0.433982   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "127          0.038028  \n",
       "128          0.038454  \n",
       "129          0.038592  \n",
       "130          0.038613  \n",
       "131          0.038632  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('ticks')\n",
    "\n",
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [6, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  1.9min finished\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.06393404, 0.16871133, 0.27317624, 0.0560194 , 0.15813637,\n",
       "        0.26875076, 0.05551591, 0.16184907, 0.2871943 , 0.06966043,\n",
       "        0.19724741, 0.32387762, 0.06641893, 0.18429575, 0.30821862,\n",
       "        0.06490488, 0.18411322, 0.30529122, 0.07761917, 0.21039138,\n",
       "        0.35157981, 0.07513819, 0.20651126, 0.35762377, 0.074441  ,\n",
       "        0.21893291, 0.42235398, 0.10234966, 0.23558497, 0.42039208,\n",
       "        0.08535228, 0.27494516, 0.40075655, 0.08082018, 0.23621688,\n",
       "        0.44507489, 0.09823298, 0.38764887, 0.55447822, 0.1099906 ,\n",
       "        0.29173098, 0.51225796, 0.11045089, 0.30029011, 0.49890418,\n",
       "        0.07568378, 0.19797864, 0.33251886, 0.07014179, 0.18968658,\n",
       "        0.31787429, 0.07867622, 0.20882602, 0.35054426, 0.08231845,\n",
       "        0.24893937, 0.36666398, 0.0825254 , 0.22608552, 0.36658521,\n",
       "        0.07708144, 0.22397037, 0.37355146, 0.08784676, 0.24860039,\n",
       "        0.41908865, 0.08938437, 0.25453925, 0.50211573, 0.11127024,\n",
       "        0.28642635, 0.45212474, 0.11129441, 0.29073124, 0.48896222,\n",
       "        0.10260897, 0.29452291, 0.52895441, 0.11520638, 0.27751403,\n",
       "        0.46486988, 0.1086143 , 0.32361026, 0.57393565, 0.12843428,\n",
       "        0.30948591, 0.50588241, 0.10325699, 0.29334626, 0.45368357,\n",
       "        0.07257543, 0.19945989, 0.31649265, 0.06608768, 0.20534997,\n",
       "        0.33880301, 0.07194328, 0.19850554, 0.32316418, 0.08318973,\n",
       "        0.24444451, 0.39664836, 0.08331347, 0.2279171 , 0.35774546,\n",
       "        0.07453442, 0.21692901, 0.35425134, 0.08454609, 0.247717  ,\n",
       "        0.40811386, 0.08452883, 0.24270473, 0.39498286, 0.08294258,\n",
       "        0.23921394, 0.39481897, 0.09216771, 0.27334981, 0.45963645,\n",
       "        0.09479461, 0.27536855, 0.4297977 , 0.09072237, 0.26200123,\n",
       "        0.44547029, 0.10842896, 0.30042615, 0.48436499, 0.11122365,\n",
       "        0.29517298, 0.48137522, 0.10048728, 0.28121166, 0.4690124 ,\n",
       "        0.06645627, 0.19767842, 0.32751455, 0.07254739, 0.201091  ,\n",
       "        0.33700924, 0.06695714, 0.19122682, 0.3407475 , 0.08077393,\n",
       "        0.28709745, 0.43860946, 0.09378238, 0.24011812, 0.44002695,\n",
       "        0.10037999, 0.27493505, 0.4527133 , 0.11174226, 0.27051635,\n",
       "        0.42379117, 0.09029536, 0.26377778, 0.43894124, 0.09367557,\n",
       "        0.28336501, 0.41517959, 0.09713435, 0.30752277, 0.57311435,\n",
       "        0.11422877, 0.2939465 , 0.44201651, 0.09588294, 0.28324227,\n",
       "        0.45522118, 0.10825124, 0.31681695, 0.55353804, 0.10803924,\n",
       "        0.39981627, 0.59415374, 0.12113433, 0.32930894, 0.50166836,\n",
       "        0.07058396, 0.21426401, 0.35991096, 0.0726028 , 0.2276608 ,\n",
       "        0.403089  , 0.09047441, 0.2083147 , 0.33013878, 0.08033133,\n",
       "        0.22259035, 0.4006813 , 0.08085146, 0.27531705, 0.44578881,\n",
       "        0.08455167, 0.29059339, 0.39545655, 0.09309006, 0.27900906,\n",
       "        0.51099586, 0.1122674 , 0.29439535, 0.43514085, 0.08652334,\n",
       "        0.2773571 , 0.43599639, 0.10108585, 0.30901036, 0.53722219,\n",
       "        0.10885634, 0.28860636, 0.5214344 , 0.10588403, 0.28383446,\n",
       "        0.5044208 , 0.11879921, 0.35557957, 0.53403344, 0.10941281,\n",
       "        0.31573997, 0.50649376, 0.09784327, 0.32514968, 0.56216102,\n",
       "        0.08432021, 0.20232439, 0.34121966, 0.07487121, 0.22586126,\n",
       "        0.38403373, 0.08537936, 0.20902896, 0.33627505, 0.08426023,\n",
       "        0.23750863, 0.4138732 , 0.09031038, 0.23309817, 0.3768105 ,\n",
       "        0.07724838, 0.22158027, 0.36559658, 0.09062862, 0.25200624,\n",
       "        0.41610384, 0.08645339, 0.24774423, 0.42762861, 0.08971586,\n",
       "        0.24557261, 0.39737744, 0.09600854, 0.28046126, 0.48451467,\n",
       "        0.10905261, 0.27762265, 0.44072719, 0.093011  , 0.25918679,\n",
       "        0.44375815, 0.10939364, 0.30659599, 0.48834872, 0.09966631,\n",
       "        0.29692945, 0.47555366, 0.09528942, 0.28591228, 0.47962112,\n",
       "        0.07283015, 0.20111341, 0.34024425, 0.07506666, 0.19922462,\n",
       "        0.34389181, 0.07368283, 0.19876313, 0.32624373, 0.08135648,\n",
       "        0.24959469, 0.38801913, 0.08102264, 0.21542144, 0.37157574,\n",
       "        0.0775547 , 0.25695758, 0.41098642, 0.0930809 , 0.27233229,\n",
       "        0.46884937, 0.10101576, 0.27395458, 0.43715291, 0.09168038,\n",
       "        0.27789207, 0.44301915, 0.09916201, 0.29403682, 0.4602437 ,\n",
       "        0.09415007, 0.27191463, 0.48422265, 0.09761219, 0.29506469,\n",
       "        0.46258693, 0.12140317, 0.31904922, 0.5343236 , 0.11433134,\n",
       "        0.29045343, 0.48770809, 0.10272651, 0.28113708, 0.46691437,\n",
       "        0.06995225, 0.19408398, 0.32656903, 0.07234435, 0.19124398,\n",
       "        0.32168317, 0.06778564, 0.19094281, 0.32140512, 0.07662277,\n",
       "        0.23078837, 0.37493725, 0.08514376, 0.2234828 , 0.38199129,\n",
       "        0.077948  , 0.23478985, 0.37657132, 0.09116583, 0.26641922,\n",
       "        0.43691478, 0.09313483, 0.26671257, 0.43179064, 0.09160399,\n",
       "        0.25141602, 0.43445735, 0.10414019, 0.28496108, 0.46891565,\n",
       "        0.10143456, 0.27279882, 0.44423933, 0.09237628, 0.2786685 ,\n",
       "        0.44379654, 0.10649657, 0.29955254, 0.50422378, 0.10169191,\n",
       "        0.29698977, 0.48598824, 0.10065989, 0.29752159, 0.46913061,\n",
       "        0.06759043, 0.19206653, 0.32242637, 0.06679654, 0.19469919,\n",
       "        0.32981558, 0.06993084, 0.18816023, 0.31674042, 0.07924037,\n",
       "        0.23052535, 0.42822847, 0.08662963, 0.25314651, 0.36627483,\n",
       "        0.07913609, 0.2277319 , 0.37017097, 0.09084144, 0.26277575,\n",
       "        0.43368902, 0.09134564, 0.26102295, 0.4307385 , 0.09078197,\n",
       "        0.26872554, 0.4320622 , 0.09926639, 0.29092689, 0.49513507,\n",
       "        0.09712262, 0.29965796, 0.47933536, 0.09305429, 0.28138094,\n",
       "        0.4509923 , 0.12183523, 0.31391273, 0.50402784, 0.10057802,\n",
       "        0.2937356 , 0.49159522, 0.10362239, 0.287643  , 0.4864481 ,\n",
       "        0.07129273, 0.19730945, 0.3285109 , 0.07117524, 0.19522662,\n",
       "        0.31764641, 0.06808538, 0.19694705, 0.32955127, 0.08162603,\n",
       "        0.2337368 , 0.38284583, 0.07976623, 0.22519164, 0.36931782,\n",
       "        0.08331776, 0.22936916, 0.38482876, 0.0891994 , 0.26122589,\n",
       "        0.4555953 , 0.09600496, 0.27456865, 0.43349714, 0.08835211,\n",
       "        0.2552834 , 0.45020485, 0.10471244, 0.3162499 , 0.51071339,\n",
       "        0.0992116 , 0.27746282, 0.48362346, 0.09815512, 0.28337045,\n",
       "        0.47156539, 0.11041446, 0.31549306, 0.54605079, 0.10288033,\n",
       "        0.30852294, 0.50731444, 0.09758067, 0.29448423, 0.47790041,\n",
       "        0.06380959, 0.17965808, 0.30142622, 0.0608192 , 0.1806334 ,\n",
       "        0.30122943, 0.06251659, 0.18048348, 0.30261064, 0.0715445 ,\n",
       "        0.21155953, 0.34264884, 0.07161679, 0.20075698, 0.33381591,\n",
       "        0.07270384, 0.20196195, 0.34741592, 0.08765674, 0.23700075,\n",
       "        0.3927855 , 0.08393402, 0.22315493, 0.37848659, 0.07933979,\n",
       "        0.23130322, 0.375771  , 0.08839378, 0.25509181, 0.41695566,\n",
       "        0.08589883, 0.25364041, 0.39957418, 0.08716507, 0.24188328,\n",
       "        0.39690256, 0.09341664, 0.31087275, 0.50823889, 0.10257854,\n",
       "        0.25917521, 0.45438647, 0.09565954, 0.27969432, 0.40438175,\n",
       "        0.06297951, 0.18448577, 0.33291984, 0.07213993, 0.21292639,\n",
       "        0.30495524, 0.06267095, 0.17921295, 0.30070343, 0.06909709,\n",
       "        0.20569606, 0.33924208, 0.0716321 , 0.21497059, 0.34103875,\n",
       "        0.07156978, 0.20607533, 0.33976974, 0.08987012, 0.25472989,\n",
       "        0.38780293, 0.07977223, 0.22508397, 0.37988162, 0.08509951,\n",
       "        0.22998724, 0.38022265, 0.09113455, 0.25402069, 0.41366606,\n",
       "        0.08902268, 0.24753156, 0.40986476, 0.08683677, 0.24633465,\n",
       "        0.45208454, 0.10624142, 0.31259327, 0.46522312, 0.09571896,\n",
       "        0.25782719, 0.43743281, 0.09602265, 0.27937512, 0.44095283,\n",
       "        0.07032328, 0.19311166, 0.30770879, 0.06589799, 0.19332795,\n",
       "        0.30605679, 0.06701488, 0.17601457, 0.3016767 , 0.07453761,\n",
       "        0.21299891, 0.42873306, 0.09395275, 0.25682712, 0.36310015,\n",
       "        0.07127776, 0.20364113, 0.35864062, 0.07966671, 0.28796415,\n",
       "        0.40095434, 0.07574997, 0.22333198, 0.37558241, 0.07821388,\n",
       "        0.22758479, 0.37402377, 0.08731089, 0.25324655, 0.41358118,\n",
       "        0.08679013, 0.24776635, 0.40809393, 0.08202353, 0.2417738 ,\n",
       "        0.40632372, 0.09558072, 0.27481418, 0.44360509, 0.09025302,\n",
       "        0.25344787, 0.43658481, 0.10254121, 0.25443726, 0.41906343,\n",
       "        0.06546364, 0.18889699, 0.31929417, 0.06531305, 0.19351411,\n",
       "        0.31390486, 0.06917272, 0.18010406, 0.30037632, 0.07253804,\n",
       "        0.20367265, 0.35496473, 0.07650895, 0.2194674 , 0.3633698 ,\n",
       "        0.07922978, 0.21779284, 0.34246869, 0.08106337, 0.24491262,\n",
       "        0.39631605, 0.0761786 , 0.2433219 , 0.41675882, 0.08504372,\n",
       "        0.21805644, 0.37019253, 0.08579473, 0.24916987, 0.39379148,\n",
       "        0.08801641, 0.25622115, 0.41100683, 0.07965755, 0.2411346 ,\n",
       "        0.41006837, 0.09575734, 0.30370822, 0.50513387, 0.10828018,\n",
       "        0.27750745, 0.46755557, 0.09195375, 0.27971873, 0.45516186,\n",
       "        0.06936126, 0.20766268, 0.33028121, 0.06760235, 0.23501024,\n",
       "        0.34662848, 0.06978164, 0.20474744, 0.33950186, 0.08083243,\n",
       "        0.25307493, 0.35235877, 0.07848153, 0.24267116, 0.39727526,\n",
       "        0.07519379, 0.23323755, 0.36583476, 0.08535309, 0.26664715,\n",
       "        0.42855277, 0.08139701, 0.2288785 , 0.38092084, 0.0780488 ,\n",
       "        0.22547336, 0.37254667, 0.08941517, 0.24812655, 0.42211347,\n",
       "        0.08709078, 0.24568882, 0.39960942, 0.08488221, 0.23422108,\n",
       "        0.41579781, 0.09337859, 0.26713495, 0.4342957 , 0.08912158,\n",
       "        0.25658636, 0.43919101, 0.08992805, 0.24296622, 0.34434423]),\n",
       " 'std_fit_time': array([0.00321573, 0.00466669, 0.0065708 , 0.00189267, 0.00305703,\n",
       "        0.00595828, 0.00199906, 0.00428114, 0.00448619, 0.00547517,\n",
       "        0.00618094, 0.02050703, 0.00179875, 0.00827812, 0.0048836 ,\n",
       "        0.00144064, 0.00268897, 0.02502035, 0.0079562 , 0.0053727 ,\n",
       "        0.00226092, 0.00250971, 0.00537771, 0.00635293, 0.00305086,\n",
       "        0.03584795, 0.00503189, 0.02902064, 0.00154707, 0.01246963,\n",
       "        0.00703908, 0.01063064, 0.00684469, 0.00204028, 0.00682555,\n",
       "        0.020529  , 0.00637754, 0.01356172, 0.02079532, 0.00751701,\n",
       "        0.00560036, 0.00301103, 0.00415437, 0.00717437, 0.03866879,\n",
       "        0.00567128, 0.01434948, 0.00590729, 0.00518313, 0.00787072,\n",
       "        0.01788377, 0.00460222, 0.00510514, 0.00223661, 0.00279126,\n",
       "        0.03266839, 0.00536013, 0.00903512, 0.0036475 , 0.00370498,\n",
       "        0.00169255, 0.00470986, 0.00308065, 0.00261272, 0.01697415,\n",
       "        0.01182297, 0.00286173, 0.00824573, 0.01407499, 0.00670498,\n",
       "        0.00593352, 0.00388605, 0.00954563, 0.00794358, 0.00697708,\n",
       "        0.00299124, 0.00916739, 0.00763255, 0.00588291, 0.01309504,\n",
       "        0.0239363 , 0.00551348, 0.00914612, 0.00685958, 0.01327192,\n",
       "        0.0044266 , 0.01284135, 0.0038484 , 0.00251766, 0.00838312,\n",
       "        0.0121876 , 0.0208595 , 0.00938623, 0.00469903, 0.01394187,\n",
       "        0.02744161, 0.00655938, 0.00277492, 0.00667012, 0.00573079,\n",
       "        0.01204145, 0.00782855, 0.00105431, 0.00516695, 0.0030719 ,\n",
       "        0.00294676, 0.00419325, 0.00808608, 0.00455453, 0.00182034,\n",
       "        0.00459848, 0.00245004, 0.00587103, 0.00505789, 0.00343153,\n",
       "        0.0025531 , 0.00458854, 0.00266203, 0.00376987, 0.00882559,\n",
       "        0.0048428 , 0.00845841, 0.00634103, 0.00365788, 0.00615962,\n",
       "        0.00415052, 0.00484594, 0.00313265, 0.00376531, 0.01776095,\n",
       "        0.02745263, 0.00694528, 0.00511089, 0.00467087, 0.01384637,\n",
       "        0.00249222, 0.00710675, 0.00240745, 0.00323002, 0.00205937,\n",
       "        0.02933053, 0.00106881, 0.01012768, 0.01379192, 0.00264572,\n",
       "        0.00644225, 0.00974956, 0.00523038, 0.00675333, 0.00712148,\n",
       "        0.01037411, 0.00183625, 0.0117936 , 0.00635304, 0.00684242,\n",
       "        0.00358981, 0.0021021 , 0.01609049, 0.00395284, 0.00408517,\n",
       "        0.03845156, 0.01186996, 0.00551506, 0.01310781, 0.00875421,\n",
       "        0.00499209, 0.01311762, 0.00874007, 0.00706804, 0.00906955,\n",
       "        0.00670627, 0.0051501 , 0.04507315, 0.04130866, 0.01080363,\n",
       "        0.01028382, 0.00964058, 0.01317485, 0.02347203, 0.01532291,\n",
       "        0.00463366, 0.02327742, 0.01119229, 0.00962435, 0.01744533,\n",
       "        0.00940995, 0.00444448, 0.00843792, 0.00568678, 0.00208242,\n",
       "        0.00556555, 0.02082985, 0.00406614, 0.00540842, 0.0067813 ,\n",
       "        0.0075287 , 0.01513647, 0.00763292, 0.00552475, 0.02319493,\n",
       "        0.00950999, 0.00382329, 0.00980214, 0.00553762, 0.00309557,\n",
       "        0.03410771, 0.00470527, 0.0042232 , 0.02253842, 0.02673109,\n",
       "        0.00734756, 0.00754705, 0.04043431, 0.00724942, 0.00733713,\n",
       "        0.01603985, 0.00736884, 0.00723753, 0.01186743, 0.01136636,\n",
       "        0.01064435, 0.00596863, 0.00298709, 0.02423811, 0.01437907,\n",
       "        0.00956122, 0.01374027, 0.01556599, 0.00620548, 0.01440101,\n",
       "        0.02073461, 0.00671609, 0.00676437, 0.00575892, 0.00142686,\n",
       "        0.00849634, 0.00312611, 0.00355916, 0.00506202, 0.00255164,\n",
       "        0.00267186, 0.00374068, 0.00161873, 0.00316189, 0.00579262,\n",
       "        0.01268171, 0.00285115, 0.00527214, 0.00576347, 0.0069567 ,\n",
       "        0.00289335, 0.00743593, 0.00549286, 0.00637538, 0.0255692 ,\n",
       "        0.0160524 , 0.00877404, 0.03331431, 0.00214525, 0.00590047,\n",
       "        0.00356425, 0.00720672, 0.01032359, 0.00820716, 0.00295104,\n",
       "        0.01644885, 0.01190929, 0.00769454, 0.01157397, 0.00375284,\n",
       "        0.00253706, 0.00367979, 0.00469717, 0.00334946, 0.00394243,\n",
       "        0.00357864, 0.00335593, 0.00360638, 0.00674896, 0.00310994,\n",
       "        0.00240765, 0.01724834, 0.00444061, 0.00409235, 0.01051452,\n",
       "        0.00361996, 0.00726781, 0.00842935, 0.0052028 , 0.00911215,\n",
       "        0.00461844, 0.00860606, 0.00889151, 0.00194934, 0.00309087,\n",
       "        0.01931556, 0.00830195, 0.00279063, 0.0080114 , 0.0044501 ,\n",
       "        0.00401925, 0.01345253, 0.03573834, 0.00608244, 0.02772668,\n",
       "        0.00470481, 0.01291793, 0.00626231, 0.00606365, 0.001817  ,\n",
       "        0.00952687, 0.00954245, 0.00442164, 0.00561442, 0.00163912,\n",
       "        0.00273537, 0.00329355, 0.00683089, 0.0036048 , 0.00888735,\n",
       "        0.00997506, 0.004257  , 0.00440271, 0.01116948, 0.00362044,\n",
       "        0.00433317, 0.00457716, 0.0073101 , 0.00347853, 0.00710921,\n",
       "        0.00302652, 0.00674331, 0.00703022, 0.00291019, 0.01895673,\n",
       "        0.00352847, 0.00233107, 0.0039567 , 0.00701377, 0.00370645,\n",
       "        0.00509999, 0.01210335, 0.00291665, 0.00531513, 0.00601173,\n",
       "        0.00274926, 0.00431433, 0.00915175, 0.0033738 , 0.01269588,\n",
       "        0.005882  , 0.00477831, 0.00742568, 0.01329124, 0.00146422,\n",
       "        0.0059812 , 0.0121261 , 0.0065689 , 0.00326921, 0.01263375,\n",
       "        0.00394223, 0.01233355, 0.01203523, 0.00392754, 0.00476197,\n",
       "        0.01942721, 0.00520251, 0.0044702 , 0.01360877, 0.00498008,\n",
       "        0.00624514, 0.00626893, 0.00436594, 0.02263347, 0.00759581,\n",
       "        0.00434462, 0.01123296, 0.00473414, 0.00187352, 0.00569339,\n",
       "        0.00193959, 0.00208103, 0.00602243, 0.00480475, 0.00460308,\n",
       "        0.00277454, 0.01884073, 0.00517251, 0.00408903, 0.00444776,\n",
       "        0.00311039, 0.00980553, 0.02009408, 0.004633  , 0.01008708,\n",
       "        0.00309062, 0.03149801, 0.00797884, 0.00669924, 0.00363633,\n",
       "        0.00374038, 0.00536477, 0.00524378, 0.00548879, 0.00752508,\n",
       "        0.00257458, 0.00337191, 0.00243357, 0.00377496, 0.00677256,\n",
       "        0.00480312, 0.00392042, 0.00166946, 0.0057318 , 0.00476671,\n",
       "        0.00706905, 0.00615808, 0.00261202, 0.00110498, 0.00771165,\n",
       "        0.00524022, 0.01399001, 0.02194215, 0.00288137, 0.00644917,\n",
       "        0.00573816, 0.0063283 , 0.01561436, 0.02138685, 0.00898414,\n",
       "        0.00620597, 0.01171762, 0.00505029, 0.0053945 , 0.00961015,\n",
       "        0.00156674, 0.00626906, 0.00854237, 0.00270183, 0.01411403,\n",
       "        0.02311817, 0.00565893, 0.01974204, 0.00658428, 0.00526176,\n",
       "        0.00926446, 0.00524823, 0.0033991 , 0.00542203, 0.0015874 ,\n",
       "        0.0030292 , 0.00821822, 0.01032266, 0.00329794, 0.01018462,\n",
       "        0.00431058, 0.00192334, 0.00533407, 0.00758873, 0.00391757,\n",
       "        0.00403015, 0.00711151, 0.00353422, 0.00498106, 0.00450075,\n",
       "        0.00294723, 0.00594096, 0.00922479, 0.00351468, 0.00397867,\n",
       "        0.00980753, 0.00278724, 0.00718617, 0.00595722, 0.00259416,\n",
       "        0.0014932 , 0.00974688, 0.00495469, 0.00815997, 0.00841164,\n",
       "        0.00186808, 0.00216823, 0.00557434, 0.0043071 , 0.00531363,\n",
       "        0.01025376, 0.00117348, 0.01076727, 0.02288261, 0.0096384 ,\n",
       "        0.00420817, 0.00997855, 0.00675744, 0.00426833, 0.00667646,\n",
       "        0.00268949, 0.00272006, 0.01365356, 0.00719806, 0.01252792,\n",
       "        0.00295547, 0.00184739, 0.00388618, 0.004901  , 0.00457053,\n",
       "        0.00497122, 0.00730419, 0.00302584, 0.00315072, 0.00674489,\n",
       "        0.00261715, 0.00404528, 0.00372815, 0.02521549, 0.00480465,\n",
       "        0.00375801, 0.00421215, 0.00319874, 0.00887333, 0.00669355,\n",
       "        0.00461003, 0.00620705, 0.00198738, 0.0075027 , 0.00749201,\n",
       "        0.00410606, 0.0030318 , 0.00613668, 0.00459802, 0.00458416,\n",
       "        0.01119856, 0.00232689, 0.0100336 , 0.00768789, 0.00587724,\n",
       "        0.00262581, 0.02173862, 0.00723824, 0.02491436, 0.012473  ,\n",
       "        0.00577972, 0.01672025, 0.00754698, 0.00271936, 0.0043193 ,\n",
       "        0.00424324, 0.0031235 , 0.0028518 , 0.00648392, 0.00302746,\n",
       "        0.02341843, 0.02843561, 0.01497099, 0.01001714, 0.01031015,\n",
       "        0.00619776, 0.00323604, 0.06577685, 0.00705007, 0.02756231,\n",
       "        0.02080265, 0.00349875, 0.01288297, 0.00555713, 0.00148021,\n",
       "        0.00607651, 0.00141654, 0.00386395, 0.00633397, 0.00726119,\n",
       "        0.00181967, 0.00384492, 0.00404218, 0.00194747, 0.00340283,\n",
       "        0.00624516, 0.00279011, 0.00414424, 0.00550782, 0.00221959,\n",
       "        0.00437109, 0.00509057, 0.0215787 , 0.01360475, 0.00859232,\n",
       "        0.00060477, 0.0035807 , 0.00209015, 0.00299403, 0.00347061,\n",
       "        0.00200789, 0.00547686, 0.00432304, 0.01105761, 0.00479045,\n",
       "        0.00688876, 0.00238019, 0.00362283, 0.0063776 , 0.01160713,\n",
       "        0.00301726, 0.00402406, 0.00483167, 0.00413021, 0.0040969 ,\n",
       "        0.007891  , 0.0034013 , 0.01363351, 0.01758639, 0.00236094,\n",
       "        0.00676005, 0.00604222, 0.00375105, 0.00786359, 0.00835494,\n",
       "        0.00843236, 0.03112025, 0.00490047, 0.00410933, 0.01335971,\n",
       "        0.00718136, 0.00419309, 0.00622147, 0.01776057, 0.00958644,\n",
       "        0.01473856, 0.00796009, 0.00354855, 0.00872702, 0.0054655 ,\n",
       "        0.00205343, 0.00931591, 0.00591462, 0.00244887, 0.00501504,\n",
       "        0.00674178, 0.00292307, 0.00527911, 0.00800246, 0.00787797,\n",
       "        0.00588456, 0.00801991, 0.01292977, 0.0103728 , 0.00829445,\n",
       "        0.0014189 , 0.00560098, 0.00409496, 0.00419842, 0.00462129,\n",
       "        0.00855118, 0.00861995, 0.00182584, 0.00393615, 0.00226384,\n",
       "        0.00298922, 0.01573453, 0.00910617, 0.00304104, 0.01067849,\n",
       "        0.00484405, 0.00784435, 0.00960719, 0.00409635, 0.00799306,\n",
       "        0.00571748, 0.00381998, 0.00810972, 0.00615936, 0.00498839,\n",
       "        0.00796358, 0.01766066, 0.00452419, 0.00427986, 0.01141344]),\n",
       " 'mean_score_time': array([0.00443654, 0.00507989, 0.00543041, 0.00342689, 0.00537081,\n",
       "        0.00584297, 0.00348725, 0.00475988, 0.00576944, 0.00557146,\n",
       "        0.00593777, 0.00594344, 0.00358639, 0.00482788, 0.00658655,\n",
       "        0.00376387, 0.0046926 , 0.00608034, 0.00374937, 0.00739288,\n",
       "        0.00849118, 0.00364237, 0.00633254, 0.00741596, 0.00366755,\n",
       "        0.00906143, 0.0068274 , 0.00435796, 0.00565572, 0.01012487,\n",
       "        0.0040803 , 0.00568914, 0.00799522, 0.00365019, 0.00609837,\n",
       "        0.00926886, 0.00519543, 0.00808196, 0.01043015, 0.00447721,\n",
       "        0.00678406, 0.01077204, 0.00523233, 0.0082653 , 0.0100986 ,\n",
       "        0.00401163, 0.00502563, 0.00620217, 0.00385222, 0.00506582,\n",
       "        0.00656676, 0.0037334 , 0.00596576, 0.00670881, 0.00413189,\n",
       "        0.00651159, 0.00807023, 0.00405707, 0.00647593, 0.00850077,\n",
       "        0.00413508, 0.00608182, 0.00694637, 0.00422883, 0.00825005,\n",
       "        0.00820093, 0.00448484, 0.00679545, 0.00989966, 0.00645275,\n",
       "        0.006879  , 0.01006446, 0.00438318, 0.00716295, 0.01003895,\n",
       "        0.00425901, 0.0097702 , 0.0091784 , 0.00405431, 0.00657535,\n",
       "        0.00893412, 0.00548358, 0.00974703, 0.01144619, 0.00532289,\n",
       "        0.00945797, 0.01006875, 0.00568919, 0.00898504, 0.00843668,\n",
       "        0.00386515, 0.00646076, 0.00629501, 0.00382404, 0.00608888,\n",
       "        0.00651999, 0.00372148, 0.00506911, 0.00688062, 0.00456324,\n",
       "        0.00723648, 0.00766773, 0.00441661, 0.00531836, 0.00743184,\n",
       "        0.00448003, 0.00637665, 0.00690584, 0.004775  , 0.00639911,\n",
       "        0.00923371, 0.00407567, 0.00573144, 0.0077085 , 0.00385919,\n",
       "        0.00704627, 0.0074007 , 0.00430064, 0.00638275, 0.01235199,\n",
       "        0.00426817, 0.00672817, 0.00936928, 0.00385647, 0.0066308 ,\n",
       "        0.00944524, 0.00513611, 0.00766077, 0.01002021, 0.00512977,\n",
       "        0.00706477, 0.00862179, 0.00567994, 0.00636573, 0.00879397,\n",
       "        0.00377541, 0.00492058, 0.00832472, 0.0035059 , 0.00502381,\n",
       "        0.00619278, 0.00419726, 0.00461235, 0.00738173, 0.00454221,\n",
       "        0.00720458, 0.00788202, 0.00467477, 0.0082181 , 0.00872149,\n",
       "        0.00423741, 0.00688729, 0.00864339, 0.00459118, 0.0069561 ,\n",
       "        0.00892382, 0.00435762, 0.0086616 , 0.00842433, 0.00432963,\n",
       "        0.00707779, 0.00818944, 0.004354  , 0.00948157, 0.01642118,\n",
       "        0.00415044, 0.00716629, 0.01094403, 0.00384827, 0.00798082,\n",
       "        0.00939827, 0.00483718, 0.00749521, 0.01456556, 0.00581551,\n",
       "        0.0077064 , 0.01218839, 0.00442343, 0.00674248, 0.01206961,\n",
       "        0.00446782, 0.00606136, 0.00615926, 0.0040338 , 0.00651088,\n",
       "        0.00775518, 0.00463686, 0.00673671, 0.00592399, 0.00440202,\n",
       "        0.00557284, 0.00943055, 0.0043684 , 0.00758224, 0.01661587,\n",
       "        0.00591283, 0.00560679, 0.00785704, 0.00454755, 0.00713701,\n",
       "        0.0103827 , 0.00491405, 0.00672817, 0.00824771, 0.0044982 ,\n",
       "        0.00623646, 0.00798588, 0.00490518, 0.00930705, 0.00932274,\n",
       "        0.00544457, 0.00650549, 0.00877943, 0.00455256, 0.00651464,\n",
       "        0.00957198, 0.00583434, 0.00784674, 0.01178479, 0.00436273,\n",
       "        0.00724406, 0.00997429, 0.00375109, 0.0120616 , 0.00961814,\n",
       "        0.00448298, 0.00592523, 0.00729299, 0.00509934, 0.00595589,\n",
       "        0.00648518, 0.00454621, 0.00613828, 0.00639338, 0.00556459,\n",
       "        0.00802765, 0.00820708, 0.00469236, 0.00677829, 0.00743933,\n",
       "        0.00436034, 0.00614958, 0.00725608, 0.00478163, 0.00674005,\n",
       "        0.00912914, 0.00392361, 0.00645041, 0.00838242, 0.00498023,\n",
       "        0.00618672, 0.00854101, 0.00458937, 0.00765595, 0.00910521,\n",
       "        0.00422683, 0.0070972 , 0.00808225, 0.00404286, 0.00693593,\n",
       "        0.00904045, 0.00475354, 0.00758181, 0.01075277, 0.00455871,\n",
       "        0.00744104, 0.01008215, 0.0050312 , 0.00632401, 0.00931997,\n",
       "        0.00365081, 0.00562978, 0.006145  , 0.00466685, 0.00597997,\n",
       "        0.00668621, 0.00438676, 0.00473361, 0.00696545, 0.00371747,\n",
       "        0.00625386, 0.00707464, 0.00453358, 0.00594273, 0.00808411,\n",
       "        0.00388422, 0.00678425, 0.00796952, 0.00520964, 0.00710549,\n",
       "        0.01095939, 0.00686951, 0.00730567, 0.00873542, 0.0051538 ,\n",
       "        0.0090517 , 0.00808582, 0.0049613 , 0.00633497, 0.00958943,\n",
       "        0.0048542 , 0.00684195, 0.01003461, 0.0044764 , 0.00662022,\n",
       "        0.00910187, 0.00536127, 0.0095696 , 0.01079783, 0.0053194 ,\n",
       "        0.00822682, 0.00973916, 0.00424833, 0.00661221, 0.00864964,\n",
       "        0.0038878 , 0.00521927, 0.00590887, 0.00369754, 0.00456457,\n",
       "        0.00738425, 0.00442076, 0.00483413, 0.00736232, 0.00411534,\n",
       "        0.00552754, 0.00731096, 0.00388417, 0.00518861, 0.00842843,\n",
       "        0.00404644, 0.00602307, 0.00778127, 0.00453219, 0.00656676,\n",
       "        0.00940175, 0.00402317, 0.0063067 , 0.00825076, 0.00469012,\n",
       "        0.00622263, 0.0084682 , 0.00505919, 0.00661383, 0.00928378,\n",
       "        0.00538745, 0.00667524, 0.0088675 , 0.00403633, 0.00610318,\n",
       "        0.00921655, 0.00440164, 0.00791421, 0.010221  , 0.00503993,\n",
       "        0.0072381 , 0.01245413, 0.0042407 , 0.00667801, 0.00824871,\n",
       "        0.00361986, 0.00494685, 0.00710044, 0.00407882, 0.00657954,\n",
       "        0.00623503, 0.00369959, 0.00615544, 0.00769405, 0.0039124 ,\n",
       "        0.00535493, 0.01017513, 0.00432744, 0.00623255, 0.00698261,\n",
       "        0.00462303, 0.00580988, 0.00744019, 0.00412159, 0.0064096 ,\n",
       "        0.00812163, 0.00500922, 0.00704651, 0.00947423, 0.00465446,\n",
       "        0.00720916, 0.00818481, 0.00465364, 0.00756683, 0.01017017,\n",
       "        0.00483017, 0.00664845, 0.00910864, 0.00452766, 0.00717173,\n",
       "        0.01030478, 0.00652218, 0.00751491, 0.0112114 , 0.00445881,\n",
       "        0.00854297, 0.0090106 , 0.0044723 , 0.00650325, 0.00929532,\n",
       "        0.00564842, 0.00453553, 0.00646853, 0.00395875, 0.00519457,\n",
       "        0.00622034, 0.00438313, 0.00510135, 0.00629005, 0.00453639,\n",
       "        0.00639038, 0.00718913, 0.00365257, 0.00543747, 0.00685682,\n",
       "        0.00462518, 0.00572166, 0.00786419, 0.00472016, 0.00622458,\n",
       "        0.00846882, 0.00398293, 0.00635829, 0.01134071, 0.00382915,\n",
       "        0.0061604 , 0.0096034 , 0.00442567, 0.00875163, 0.01044326,\n",
       "        0.00505199, 0.0064074 , 0.00881419, 0.00508943, 0.00654092,\n",
       "        0.01104994, 0.00438085, 0.00753303, 0.01140037, 0.00399652,\n",
       "        0.00726728, 0.01028161, 0.0049902 , 0.0079608 , 0.01309643,\n",
       "        0.00442281, 0.00563269, 0.00618968, 0.00339994, 0.00554819,\n",
       "        0.00610194, 0.00406532, 0.00559278, 0.00780511, 0.00363631,\n",
       "        0.0055047 , 0.00712199, 0.00356455, 0.0069983 , 0.00802994,\n",
       "        0.00366454, 0.00586686, 0.00760131, 0.00493083, 0.0075491 ,\n",
       "        0.00935564, 0.00434098, 0.006253  , 0.00824785, 0.00454822,\n",
       "        0.00608058, 0.00773931, 0.00525737, 0.00708275, 0.00985775,\n",
       "        0.00435734, 0.00714231, 0.01117764, 0.00417042, 0.00641551,\n",
       "        0.01046743, 0.00466456, 0.01025529, 0.01133265, 0.00445867,\n",
       "        0.00713301, 0.00916982, 0.00531106, 0.00672655, 0.00827274,\n",
       "        0.00392761, 0.00493956, 0.00856752, 0.0060904 , 0.0054503 ,\n",
       "        0.00669665, 0.0039753 , 0.00521398, 0.00661917, 0.00399623,\n",
       "        0.00543895, 0.00832596, 0.00368538, 0.00740867, 0.00768065,\n",
       "        0.00437436, 0.00529461, 0.00915771, 0.00492902, 0.00658379,\n",
       "        0.00916409, 0.00373096, 0.00626903, 0.00833416, 0.00536923,\n",
       "        0.00611486, 0.0078207 , 0.00438185, 0.00809927, 0.00948977,\n",
       "        0.00447822, 0.0075666 , 0.00909066, 0.00458684, 0.00691576,\n",
       "        0.01208816, 0.00573907, 0.00936999, 0.01068363, 0.00450149,\n",
       "        0.00672302, 0.01217222, 0.00429492, 0.00841622, 0.00895925,\n",
       "        0.00440888, 0.00576138, 0.00672116, 0.00349994, 0.00586481,\n",
       "        0.00710278, 0.00444679, 0.004948  , 0.00642715, 0.00409322,\n",
       "        0.00706582, 0.00910683, 0.00613503, 0.00873132, 0.00813518,\n",
       "        0.0036829 , 0.00592251, 0.00887918, 0.00432615, 0.00771427,\n",
       "        0.00822968, 0.00399141, 0.00729375, 0.00848908, 0.00453296,\n",
       "        0.00789623, 0.00873828, 0.00471511, 0.00821686, 0.01032763,\n",
       "        0.00423379, 0.00679479, 0.0093451 , 0.00413547, 0.00670009,\n",
       "        0.00841684, 0.00425301, 0.00843453, 0.01015501, 0.00533648,\n",
       "        0.00754132, 0.00898981, 0.00564504, 0.00650845, 0.00968151,\n",
       "        0.00463133, 0.00536013, 0.00624166, 0.00359941, 0.00516176,\n",
       "        0.00647378, 0.0039022 , 0.00506477, 0.00700507, 0.00430255,\n",
       "        0.00647688, 0.00712981, 0.00373921, 0.00563936, 0.00756378,\n",
       "        0.00435514, 0.00699   , 0.0097002 , 0.00386982, 0.00849872,\n",
       "        0.00977712, 0.00423546, 0.0090373 , 0.00783582, 0.00415154,\n",
       "        0.00576081, 0.01013441, 0.00474825, 0.00688801, 0.00971956,\n",
       "        0.00481281, 0.00717902, 0.00967298, 0.00446486, 0.00635815,\n",
       "        0.00935841, 0.00464783, 0.00881152, 0.01127634, 0.00423183,\n",
       "        0.00764256, 0.00882144, 0.00440154, 0.00716562, 0.01088977,\n",
       "        0.00397472, 0.0063653 , 0.00869622, 0.00413637, 0.00615206,\n",
       "        0.00663066, 0.00513978, 0.0080617 , 0.00905476, 0.00529623,\n",
       "        0.00792527, 0.00754743, 0.00457387, 0.00696087, 0.00771255,\n",
       "        0.00456185, 0.00586424, 0.01027141, 0.0042388 , 0.00656571,\n",
       "        0.00818305, 0.00447001, 0.00748882, 0.00931182, 0.00440083,\n",
       "        0.00557327, 0.00862069, 0.0039072 , 0.00704384, 0.00904837,\n",
       "        0.00503964, 0.00939064, 0.00896759, 0.00422859, 0.00667706,\n",
       "        0.00892787, 0.00606961, 0.00652485, 0.00920105, 0.00467267,\n",
       "        0.00625482, 0.00862722, 0.00494442, 0.00586987, 0.00720763]),\n",
       " 'std_score_time': array([8.55128010e-04, 7.66325079e-04, 3.62734524e-04, 4.63222833e-04,\n",
       "        1.17441357e-03, 3.80750437e-04, 5.22524755e-04, 7.42246604e-04,\n",
       "        5.76377029e-04, 4.62936834e-03, 7.66922573e-04, 1.68195654e-04,\n",
       "        5.58129402e-04, 4.67984434e-04, 8.80996777e-04, 9.56940047e-04,\n",
       "        3.48098487e-04, 5.49933121e-04, 4.63181634e-04, 4.27733910e-03,\n",
       "        1.75785688e-03, 1.33453372e-04, 6.57903195e-04, 7.16963040e-04,\n",
       "        1.78537498e-04, 6.57127512e-03, 5.58803030e-04, 5.57656043e-04,\n",
       "        5.10463975e-04, 1.63793787e-03, 8.69755538e-04, 5.10510066e-04,\n",
       "        7.88823722e-04, 3.37786744e-04, 1.27962284e-03, 1.26472780e-03,\n",
       "        1.87967237e-03, 1.34699006e-03, 1.70664265e-03, 5.04444436e-04,\n",
       "        7.54548972e-04, 1.60647783e-03, 1.79055801e-03, 2.23492157e-03,\n",
       "        5.84129559e-04, 5.52597549e-04, 6.48681093e-04, 3.64286656e-04,\n",
       "        3.53612534e-04, 6.95311456e-04, 6.29326964e-04, 4.17701866e-04,\n",
       "        1.49448479e-03, 7.76593726e-04, 5.28951670e-04, 8.36307108e-04,\n",
       "        8.16439379e-04, 3.88667672e-04, 1.54317509e-03, 2.76758566e-03,\n",
       "        6.03888638e-04, 8.31530889e-04, 7.31555456e-04, 4.74831488e-04,\n",
       "        4.46320429e-03, 9.07002190e-04, 5.40854118e-04, 1.26408038e-03,\n",
       "        3.90429437e-03, 2.56448941e-03, 1.05822830e-03, 3.35363798e-03,\n",
       "        7.82451675e-04, 2.00275473e-03, 2.31427972e-04, 5.28434729e-04,\n",
       "        2.33270901e-03, 1.05234641e-03, 2.31593529e-04, 1.18029647e-03,\n",
       "        1.10794057e-03, 1.55510368e-03, 3.00064696e-03, 1.88622747e-03,\n",
       "        1.55461073e-03, 1.87909256e-03, 1.72030982e-03, 1.35180588e-03,\n",
       "        1.53327655e-03, 3.81191241e-04, 8.40758252e-04, 1.33325420e-03,\n",
       "        7.63223110e-04, 5.01124261e-04, 1.52577867e-03, 6.23323868e-04,\n",
       "        1.59294017e-04, 5.53570351e-04, 9.02854141e-04, 1.15030117e-03,\n",
       "        1.21271378e-03, 6.61986338e-04, 7.50868398e-04, 4.86535518e-04,\n",
       "        8.74261464e-04, 1.67582567e-03, 1.31456421e-03, 7.24044203e-04,\n",
       "        1.54949403e-03, 9.95478757e-04, 1.65924448e-03, 3.82967708e-04,\n",
       "        2.65426023e-04, 6.44073349e-04, 3.96909483e-04, 2.28051479e-03,\n",
       "        2.35646845e-04, 5.08048911e-04, 2.57950366e-04, 4.51530158e-03,\n",
       "        4.69606630e-04, 8.00299528e-04, 1.87352797e-03, 1.71417116e-04,\n",
       "        1.05962854e-03, 1.82642159e-03, 1.60084296e-03, 1.17886182e-03,\n",
       "        9.56220463e-04, 2.33118501e-03, 7.78790665e-04, 1.48523148e-04,\n",
       "        1.14914859e-03, 2.85331498e-04, 1.34831543e-03, 3.88884438e-04,\n",
       "        6.53607332e-04, 4.06971873e-03, 1.87698963e-04, 4.84205550e-04,\n",
       "        5.57493600e-04, 5.84670467e-04, 1.85126007e-04, 1.78267492e-03,\n",
       "        1.31692022e-03, 1.49610949e-03, 9.95256438e-04, 8.43309245e-04,\n",
       "        4.14456842e-03, 2.21338977e-03, 4.44824617e-04, 2.47973859e-03,\n",
       "        2.15639680e-03, 6.41482906e-04, 1.93765189e-03, 1.55921654e-03,\n",
       "        5.63329804e-04, 2.11948326e-03, 9.33983942e-04, 6.14809642e-04,\n",
       "        1.39362744e-03, 8.41942257e-04, 3.86242101e-04, 2.73577200e-03,\n",
       "        7.83070742e-03, 2.26004737e-04, 2.26776926e-03, 3.02303702e-03,\n",
       "        9.18082308e-05, 9.35080318e-04, 7.58623963e-04, 8.56214630e-04,\n",
       "        8.15816545e-04, 5.76969882e-03, 1.69830050e-03, 9.49592761e-04,\n",
       "        2.28439913e-03, 3.32449088e-04, 7.88963961e-04, 2.96777672e-03,\n",
       "        1.43802228e-03, 1.53645275e-03, 2.72158892e-04, 9.61882991e-04,\n",
       "        1.35434920e-03, 2.19873606e-03, 2.06378069e-03, 3.09163563e-03,\n",
       "        2.39295681e-04, 4.73002791e-04, 4.94408313e-04, 1.81440805e-03,\n",
       "        3.49361069e-04, 4.22193816e-03, 8.04273002e-03, 4.14725020e-03,\n",
       "        5.80107838e-04, 8.14548991e-04, 6.67113221e-04, 5.55239195e-04,\n",
       "        2.79439590e-03, 9.59961042e-04, 6.80987655e-04, 7.13805088e-04,\n",
       "        7.91222226e-04, 4.42800100e-04, 3.57608076e-04, 7.68328194e-04,\n",
       "        3.91955876e-03, 7.21291264e-04, 1.43398396e-03, 3.55129960e-04,\n",
       "        7.73776288e-04, 3.04373431e-04, 5.24033854e-04, 1.23782877e-03,\n",
       "        2.18405225e-03, 1.29614593e-03, 2.43853033e-03, 6.76998993e-04,\n",
       "        6.61335723e-04, 5.27810389e-04, 3.71851668e-05, 6.25378240e-03,\n",
       "        1.40418539e-03, 9.69032504e-04, 1.23192741e-03, 9.61107871e-04,\n",
       "        1.38500352e-03, 6.34542588e-04, 8.18663377e-04, 1.36082335e-03,\n",
       "        3.88416089e-04, 4.02310379e-04, 1.46963156e-03, 3.24869384e-03,\n",
       "        1.42865108e-03, 1.07106680e-03, 1.86921384e-03, 9.58632994e-04,\n",
       "        1.01599664e-03, 9.04441342e-04, 1.13819171e-03, 7.25605324e-04,\n",
       "        7.16997956e-04, 1.35478163e-03, 1.30670731e-04, 3.35234327e-04,\n",
       "        5.60643154e-04, 7.11062675e-04, 6.15324515e-04, 1.55836731e-03,\n",
       "        8.19027021e-04, 1.34850795e-03, 8.02443524e-04, 8.22620866e-04,\n",
       "        1.31389065e-03, 1.48414108e-04, 4.27852912e-04, 9.78108025e-04,\n",
       "        1.17508935e-03, 6.09119024e-04, 1.45419935e-03, 1.78638812e-03,\n",
       "        4.83156121e-04, 1.54467438e-03, 1.54274526e-03, 1.15613268e-03,\n",
       "        7.37091440e-04, 8.59437786e-04, 5.43896725e-04, 1.27358726e-03,\n",
       "        6.28544415e-04, 1.15839520e-03, 7.66765988e-04, 3.05814113e-04,\n",
       "        6.70280846e-04, 2.18880742e-04, 1.33263443e-03, 1.16032265e-04,\n",
       "        7.46783115e-04, 5.21788119e-04, 1.48456536e-03, 5.60749704e-04,\n",
       "        1.13066855e-03, 2.23580274e-04, 1.47822686e-03, 7.47462355e-04,\n",
       "        1.17824524e-03, 1.86610237e-03, 3.66006337e-03, 3.98686387e-03,\n",
       "        1.18917265e-03, 5.93647650e-04, 1.33517964e-03, 2.95983485e-03,\n",
       "        1.21686837e-03, 9.78701540e-04, 3.39542525e-04, 1.00870119e-03,\n",
       "        9.85933282e-04, 9.38948104e-04, 2.15481686e-03, 6.18826004e-04,\n",
       "        1.20200558e-03, 1.71075911e-03, 1.66676495e-03, 2.77599224e-03,\n",
       "        1.30208775e-03, 9.98986108e-04, 1.99122079e-03, 4.08226889e-04,\n",
       "        2.06081352e-04, 8.83471587e-04, 5.93050090e-04, 4.08932294e-04,\n",
       "        9.03112878e-04, 4.41229010e-04, 4.37207531e-04, 1.42165894e-04,\n",
       "        2.19550614e-03, 9.74792361e-04, 5.86510662e-04, 1.90120684e-03,\n",
       "        5.30595821e-04, 3.63075329e-04, 3.85800738e-04, 6.32187111e-04,\n",
       "        1.39561170e-04, 1.11230802e-03, 2.80182998e-04, 6.85760016e-04,\n",
       "        1.14341099e-03, 1.05818485e-03, 9.03464384e-04, 1.20502736e-03,\n",
       "        1.32481857e-04, 4.01271244e-04, 6.38418169e-04, 1.04121899e-03,\n",
       "        9.33679077e-04, 1.30804805e-03, 2.01888221e-03, 2.25127372e-04,\n",
       "        7.12773450e-04, 1.33657808e-03, 6.06874400e-04, 1.34084286e-04,\n",
       "        2.59386336e-04, 3.38239947e-04, 5.04645981e-04, 5.07794262e-04,\n",
       "        1.29450370e-03, 1.11790868e-03, 1.52897020e-03, 9.21973325e-04,\n",
       "        2.47933460e-03, 3.46141785e-04, 6.63176792e-04, 6.24113367e-04,\n",
       "        2.57446735e-04, 4.36842609e-04, 7.44394436e-04, 9.67870521e-04,\n",
       "        1.03614705e-03, 7.95934788e-04, 3.08680430e-04, 2.22377120e-03,\n",
       "        1.36922064e-03, 3.74407284e-04, 2.43578175e-04, 1.77599471e-03,\n",
       "        5.96892093e-04, 1.04532028e-03, 2.05737381e-04, 1.28814794e-03,\n",
       "        5.66265790e-04, 9.12784309e-04, 4.65980568e-04, 3.99384936e-04,\n",
       "        5.07641242e-04, 2.18101723e-03, 1.09005289e-03, 7.13703050e-04,\n",
       "        5.61968308e-04, 1.09392412e-03, 5.80436004e-04, 7.92502731e-04,\n",
       "        7.96930406e-04, 1.77292791e-03, 1.25683091e-03, 3.26514557e-04,\n",
       "        5.68156979e-04, 5.28844948e-04, 1.14418724e-03, 3.66817010e-03,\n",
       "        3.54885536e-03, 1.05166525e-03, 1.26699606e-03, 5.58582617e-04,\n",
       "        1.89813415e-03, 3.59278981e-04, 9.37280225e-04, 4.56068845e-04,\n",
       "        1.04117625e-03, 2.59147131e-03, 1.22213801e-04, 9.19572578e-04,\n",
       "        9.27781967e-04, 5.67765293e-04, 5.85900937e-04, 1.32062513e-03,\n",
       "        4.90919836e-04, 7.02064508e-04, 7.17411306e-04, 1.28063284e-03,\n",
       "        5.42359761e-04, 1.05394928e-04, 3.16472357e-04, 7.90669630e-05,\n",
       "        1.02317632e-03, 7.59751343e-04, 7.87655813e-04, 1.14566920e-03,\n",
       "        4.10158749e-04, 7.10931060e-04, 1.53987589e-04, 4.45435450e-04,\n",
       "        3.66261115e-03, 1.72192995e-04, 3.56794677e-04, 1.24429062e-03,\n",
       "        6.58853368e-04, 1.94299421e-03, 2.22860073e-03, 8.85676490e-04,\n",
       "        2.11204608e-04, 4.41968645e-04, 1.21994976e-03, 9.88498261e-04,\n",
       "        3.88622121e-03, 2.17429305e-04, 6.64491963e-04, 2.82106894e-03,\n",
       "        2.66989492e-04, 9.00674840e-04, 1.34141958e-03, 4.43126482e-04,\n",
       "        1.21636210e-03, 3.23997194e-03, 8.12911934e-04, 1.45534556e-03,\n",
       "        4.58335782e-04, 2.68275366e-04, 8.43872009e-04, 5.68577668e-04,\n",
       "        6.41249015e-04, 7.48913151e-04, 2.15111811e-03, 1.05308102e-04,\n",
       "        3.57185491e-04, 3.57895668e-04, 1.83633723e-04, 1.85203322e-03,\n",
       "        9.78339326e-04, 2.03760134e-04, 1.01987080e-03, 9.52119602e-04,\n",
       "        1.24939626e-03, 1.81212859e-03, 1.87223406e-03, 5.46781761e-04,\n",
       "        3.12022358e-04, 6.56545688e-04, 9.24369421e-04, 6.52457895e-04,\n",
       "        4.30443784e-04, 1.15225862e-03, 7.62974253e-04, 1.29579477e-03,\n",
       "        6.39706807e-04, 9.29986122e-04, 3.59375529e-03, 3.60451865e-04,\n",
       "        9.03131096e-04, 1.47420741e-03, 5.00099996e-04, 4.10476335e-03,\n",
       "        2.48261422e-03, 3.25768048e-04, 7.31058080e-04, 5.03135558e-04,\n",
       "        6.22899270e-04, 9.76775126e-04, 6.83236827e-04, 6.27773597e-04,\n",
       "        7.38189077e-04, 4.31197751e-03, 1.49872067e-03, 1.36715792e-03,\n",
       "        1.41169723e-03, 5.83678534e-04, 3.75345270e-04, 5.86230114e-04,\n",
       "        9.01602339e-04, 1.91706839e-04, 8.12099019e-04, 3.30421198e-04,\n",
       "        2.20432559e-03, 1.39633025e-03, 1.08746367e-03, 5.12308024e-04,\n",
       "        1.78782836e-03, 1.93499148e-03, 1.33922965e-03, 1.59169499e-03,\n",
       "        5.33229748e-05, 4.80136316e-04, 4.53565414e-04, 1.80290108e-03,\n",
       "        6.39137977e-04, 3.50629831e-04, 3.39586752e-04, 2.25013779e-03,\n",
       "        9.55790913e-04, 9.11921426e-04, 1.16310834e-03, 6.41885074e-04,\n",
       "        1.08560921e-03, 6.73877120e-04, 1.35226963e-03, 1.31407283e-03,\n",
       "        1.66381393e-03, 1.54821915e-03, 5.15571401e-04, 4.88643530e-04,\n",
       "        3.57020175e-03, 2.45411061e-04, 1.81684588e-03, 1.34417537e-03,\n",
       "        8.98873602e-04, 1.69798421e-03, 9.40894600e-04, 1.70333642e-04,\n",
       "        1.15037202e-03, 1.62796642e-03, 1.82841462e-03, 5.48478769e-04,\n",
       "        5.40436633e-04, 4.73846340e-04, 1.83632747e-03, 2.54900976e-03,\n",
       "        2.82413634e-03, 5.12688294e-03, 7.01454034e-04, 1.30068329e-04,\n",
       "        1.49679648e-03, 2.21122783e-03, 2.49857912e-04, 1.96592085e-03,\n",
       "        8.12678416e-04, 3.28951926e-04, 3.26112338e-03, 1.22026278e-03,\n",
       "        7.23925272e-04, 1.79451129e-03, 7.24241336e-04, 9.63711167e-04,\n",
       "        1.10508835e-03, 1.64757527e-03, 6.66042848e-04, 9.45486812e-04,\n",
       "        9.43910584e-04, 3.74896021e-04, 9.23529480e-04, 5.39349778e-04,\n",
       "        1.50607535e-04, 3.29975695e-03, 9.33216922e-04, 1.58701288e-03,\n",
       "        1.07220508e-03, 3.68165010e-04, 2.82268568e-03, 2.00724294e-04,\n",
       "        1.24248383e-03, 8.11357690e-04, 8.14563724e-04, 4.79937715e-04,\n",
       "        5.21390416e-05, 4.14054703e-04, 1.43555374e-03, 4.04619411e-04,\n",
       "        5.40679920e-04, 9.75759062e-04, 7.49397957e-04, 1.62952435e-03,\n",
       "        5.61720985e-04, 8.92526057e-05, 8.17574893e-04, 6.82967977e-04,\n",
       "        8.68482242e-04, 2.10080429e-03, 1.34774768e-03, 1.19535325e-04,\n",
       "        1.05727198e-03, 2.21266677e-03, 1.03055531e-03, 3.60726824e-03,\n",
       "        6.74536099e-04, 2.90956930e-04, 2.86658525e-04, 2.61637310e-03,\n",
       "        1.12155426e-03, 8.85234354e-04, 6.03379336e-04, 1.58552053e-03,\n",
       "        2.67695540e-03, 1.78827172e-03, 1.14639845e-03, 4.82249679e-04,\n",
       "        1.26711178e-03, 7.50486770e-04, 1.57213374e-03, 1.91900543e-03,\n",
       "        1.84268449e-04, 1.52935112e-03, 2.41685063e-04, 3.18286199e-04,\n",
       "        9.48640789e-04, 1.66124439e-03, 2.28960074e-04, 1.64570187e-03,\n",
       "        7.52460059e-04, 4.32335038e-04, 1.50391560e-03, 6.72705244e-04,\n",
       "        1.16476762e-03, 2.75660415e-03, 2.05127445e-03, 1.04310601e-03,\n",
       "        4.80647887e-03, 5.90874346e-04, 1.11333632e-03, 1.53059936e-03,\n",
       "        1.43672816e-03, 1.11974975e-03, 8.55583222e-04, 2.59590264e-03,\n",
       "        5.14952637e-04, 7.24510772e-04, 5.83571841e-04, 1.14287540e-03,\n",
       "        1.69662443e-03, 1.61244673e-03, 5.09758339e-04, 2.31156913e-04,\n",
       "        7.53146659e-04, 2.85623786e-04, 7.06420965e-04, 1.10254971e-03,\n",
       "        1.32835370e-03, 3.54473274e-03, 9.45100370e-04, 4.68900724e-04,\n",
       "        7.85983325e-04, 6.29704197e-04, 2.82586078e-03, 9.98226143e-04,\n",
       "        6.19613348e-04, 5.64271102e-04, 3.13798292e-04, 3.56569071e-04,\n",
       "        1.08418341e-03, 4.02567918e-04, 2.78604326e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82352941, 0.85148515, 0.84      , 0.86      , 0.87378641,\n",
       "        0.85714286, 0.85436893, 0.85714286, 0.85714286, 0.81553398,\n",
       "        0.83018868, 0.81904762, 0.84615385, 0.85714286, 0.85714286,\n",
       "        0.83495146, 0.86538462, 0.84615385, 0.83495146, 0.81553398,\n",
       "        0.80392157, 0.85436893, 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.83495146, 0.82692308, 0.83809524, 0.7961165 ,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.84      , 0.86538462,\n",
       "        0.84615385, 0.85148515, 0.83168317, 0.80769231, 0.8627451 ,\n",
       "        0.8627451 , 0.87128713, 0.85436893, 0.8411215 , 0.82692308,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.73469388, 0.72164948, 0.7       , 0.72164948, 0.70833333,\n",
       "        0.72727273, 0.72164948, 0.71428571, 0.73469388, 0.70212766,\n",
       "        0.69387755, 0.70588235, 0.72340426, 0.72727273, 0.72      ,\n",
       "        0.71578947, 0.72916667, 0.75      , 0.69473684, 0.71428571,\n",
       "        0.67307692, 0.69473684, 0.69387755, 0.71287129, 0.72916667,\n",
       "        0.73469388, 0.76      , 0.70833333, 0.71287129, 0.69230769,\n",
       "        0.67368421, 0.74      , 0.7254902 , 0.72164948, 0.74226804,\n",
       "        0.76767677, 0.72164948, 0.70588235, 0.69902913, 0.68817204,\n",
       "        0.73267327, 0.7184466 , 0.70833333, 0.76      , 0.76767677,\n",
       "        0.72164948, 0.70103093, 0.71428571, 0.76767677, 0.72164948,\n",
       "        0.71428571, 0.74226804, 0.72916667, 0.72164948, 0.70212766,\n",
       "        0.70833333, 0.69387755, 0.71578947, 0.72727273, 0.69387755,\n",
       "        0.70833333, 0.70833333, 0.73469388, 0.66666667, 0.70833333,\n",
       "        0.72      , 0.65957447, 0.70833333, 0.72      , 0.71578947,\n",
       "        0.73469388, 0.75510204, 0.66666667, 0.72727273, 0.70588235,\n",
       "        0.69473684, 0.70103093, 0.74      , 0.71578947, 0.73469388,\n",
       "        0.75510204, 0.66666667, 0.72727273, 0.7184466 , 0.69387755,\n",
       "        0.72727273, 0.75247525, 0.71578947, 0.74226804, 0.7755102 ,\n",
       "        0.69387755, 0.71428571, 0.72727273, 0.72164948, 0.70833333,\n",
       "        0.74747475, 0.72164948, 0.73469388, 0.71428571, 0.69473684,\n",
       "        0.72916667, 0.72164948, 0.70833333, 0.70103093, 0.72164948,\n",
       "        0.70833333, 0.70833333, 0.70833333, 0.68041237, 0.70833333,\n",
       "        0.73267327, 0.67368421, 0.6875    , 0.69387755, 0.71578947,\n",
       "        0.70833333, 0.70833333, 0.69473684, 0.72164948, 0.72      ,\n",
       "        0.67368421, 0.68085106, 0.72      , 0.69473684, 0.70103093,\n",
       "        0.74747475, 0.68085106, 0.70833333, 0.72      , 0.68085106,\n",
       "        0.69473684, 0.72727273, 0.69473684, 0.70103093, 0.74747475,\n",
       "        0.70833333, 0.72164948, 0.72164948, 0.68817204, 0.72727273,\n",
       "        0.70833333, 0.70833333, 0.72164948, 0.70833333, 0.68085106,\n",
       "        0.69473684, 0.70833333, 0.68085106, 0.70103093, 0.71578947,\n",
       "        0.70212766, 0.70103093, 0.70833333, 0.69473684, 0.6875    ,\n",
       "        0.70833333, 0.6875    , 0.6875    , 0.71578947, 0.70103093,\n",
       "        0.70833333, 0.6875    , 0.68085106, 0.69473684, 0.69473684,\n",
       "        0.67368421, 0.67368421, 0.70833333, 0.70833333, 0.70103093,\n",
       "        0.6875    , 0.69473684, 0.69473684, 0.74747475, 0.66666667,\n",
       "        0.67368421, 0.6875    , 0.70103093, 0.70103093, 0.69473684,\n",
       "        0.67391304, 0.66666667, 0.70103093, 0.66666667, 0.65957447,\n",
       "        0.71428571, 0.66666667, 0.68041237, 0.6875    , 0.65957447,\n",
       "        0.67368421, 0.6875    , 0.65263158, 0.65957447, 0.70103093,\n",
       "        0.66666667, 0.65979381, 0.70103093, 0.66666667, 0.68041237,\n",
       "        0.69387755, 0.66666667, 0.6875    , 0.71428571, 0.6875    ,\n",
       "        0.67346939, 0.70103093, 0.69473684, 0.68686869, 0.69387755,\n",
       "        0.66666667, 0.66666667, 0.6875    , 0.6875    , 0.68041237,\n",
       "        0.6875    , 0.67368421, 0.68041237, 0.68041237, 0.65979381,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.67346939, 0.69387755,\n",
       "        0.73469388, 0.72164948, 0.7       , 0.72164948, 0.70833333,\n",
       "        0.72727273, 0.72164948, 0.71428571, 0.73469388, 0.70212766,\n",
       "        0.69387755, 0.70588235, 0.72340426, 0.72727273, 0.72      ,\n",
       "        0.71578947, 0.72916667, 0.75      , 0.69473684, 0.71428571,\n",
       "        0.67307692, 0.69473684, 0.69387755, 0.71287129, 0.72916667,\n",
       "        0.73469388, 0.76      , 0.70833333, 0.71287129, 0.69230769,\n",
       "        0.67368421, 0.74      , 0.7254902 , 0.72164948, 0.74226804,\n",
       "        0.76767677, 0.72164948, 0.70588235, 0.69902913, 0.68817204,\n",
       "        0.73267327, 0.7184466 , 0.70833333, 0.76      , 0.76767677,\n",
       "        0.72164948, 0.70103093, 0.71428571, 0.76767677, 0.72164948,\n",
       "        0.71428571, 0.74226804, 0.72916667, 0.72164948, 0.70212766,\n",
       "        0.70833333, 0.69387755, 0.71578947, 0.72727273, 0.69387755,\n",
       "        0.70833333, 0.70833333, 0.73469388, 0.66666667, 0.70833333,\n",
       "        0.72      , 0.65957447, 0.70833333, 0.72      , 0.71578947,\n",
       "        0.73469388, 0.75510204, 0.66666667, 0.72727273, 0.70588235,\n",
       "        0.69473684, 0.70103093, 0.74      , 0.71578947, 0.73469388,\n",
       "        0.75510204, 0.66666667, 0.72727273, 0.7184466 , 0.69387755,\n",
       "        0.72727273, 0.75247525, 0.71578947, 0.74226804, 0.7755102 ,\n",
       "        0.69387755, 0.71428571, 0.72727273, 0.72164948, 0.70833333,\n",
       "        0.74747475, 0.72164948, 0.73469388, 0.71428571, 0.69473684,\n",
       "        0.72916667, 0.72164948, 0.70833333, 0.70103093, 0.72164948,\n",
       "        0.70833333, 0.70833333, 0.70833333, 0.68041237, 0.70833333,\n",
       "        0.73267327, 0.67368421, 0.6875    , 0.69387755, 0.71578947,\n",
       "        0.70833333, 0.70833333, 0.69473684, 0.72164948, 0.72      ,\n",
       "        0.67368421, 0.68085106, 0.72      , 0.69473684, 0.70103093,\n",
       "        0.74747475, 0.68085106, 0.70833333, 0.72      , 0.68085106,\n",
       "        0.69473684, 0.72727273, 0.69473684, 0.70103093, 0.74747475,\n",
       "        0.70833333, 0.72164948, 0.72164948, 0.68817204, 0.72727273,\n",
       "        0.70833333, 0.70833333, 0.72164948, 0.70833333, 0.68085106,\n",
       "        0.69473684, 0.70833333, 0.68085106, 0.70103093, 0.71578947,\n",
       "        0.70212766, 0.70103093, 0.70833333, 0.69473684, 0.6875    ,\n",
       "        0.70833333, 0.6875    , 0.6875    , 0.71578947, 0.70103093,\n",
       "        0.70833333, 0.6875    , 0.68085106, 0.69473684, 0.69473684,\n",
       "        0.67368421, 0.67368421, 0.70833333, 0.70833333, 0.70103093,\n",
       "        0.6875    , 0.69473684, 0.69473684, 0.74747475, 0.66666667,\n",
       "        0.67368421, 0.6875    , 0.70103093, 0.70103093, 0.69473684,\n",
       "        0.67391304, 0.66666667, 0.70103093, 0.66666667, 0.65957447,\n",
       "        0.71428571, 0.66666667, 0.68041237, 0.6875    , 0.65957447,\n",
       "        0.67368421, 0.6875    , 0.65263158, 0.65957447, 0.70103093,\n",
       "        0.66666667, 0.65979381, 0.70103093, 0.66666667, 0.68041237,\n",
       "        0.69387755, 0.66666667, 0.6875    , 0.71428571, 0.6875    ,\n",
       "        0.67346939, 0.70103093, 0.69473684, 0.68686869, 0.69387755,\n",
       "        0.66666667, 0.66666667, 0.6875    , 0.6875    , 0.68041237,\n",
       "        0.6875    , 0.67368421, 0.68041237, 0.68041237, 0.65979381,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.67346939, 0.69387755,\n",
       "        0.72      , 0.72727273, 0.71428571, 0.70103093, 0.6875    ,\n",
       "        0.69387755, 0.70103093, 0.71428571, 0.72164948, 0.6875    ,\n",
       "        0.71428571, 0.70588235, 0.70103093, 0.6875    , 0.70707071,\n",
       "        0.68041237, 0.68817204, 0.72916667, 0.6875    , 0.68      ,\n",
       "        0.70588235, 0.6875    , 0.6875    , 0.68686869, 0.70212766,\n",
       "        0.71578947, 0.74226804, 0.68041237, 0.71287129, 0.69902913,\n",
       "        0.65957447, 0.68041237, 0.73076923, 0.70833333, 0.72916667,\n",
       "        0.75510204, 0.69387755, 0.71287129, 0.68627451, 0.67368421,\n",
       "        0.72727273, 0.74509804, 0.6875    , 0.70103093, 0.72727273,\n",
       "        0.71428571, 0.72727273, 0.72727273, 0.69387755, 0.70833333,\n",
       "        0.72164948, 0.6875    , 0.70103093, 0.70103093, 0.70103093,\n",
       "        0.72916667, 0.69387755, 0.69387755, 0.70833333, 0.70833333,\n",
       "        0.6875    , 0.6875    , 0.70833333, 0.66666667, 0.70103093,\n",
       "        0.66666667, 0.69387755, 0.72164948, 0.70103093, 0.6875    ,\n",
       "        0.6875    , 0.69473684, 0.69387755, 0.71428571, 0.70588235,\n",
       "        0.6875    , 0.70103093, 0.7254902 , 0.6875    , 0.68085106,\n",
       "        0.72164948, 0.65306122, 0.7254902 , 0.69306931, 0.67368421,\n",
       "        0.70103093, 0.7254902 , 0.6875    , 0.67368421, 0.70103093,\n",
       "        0.67346939, 0.71428571, 0.74      , 0.67346939, 0.71428571,\n",
       "        0.70833333, 0.6875    , 0.71428571, 0.72164948, 0.68041237,\n",
       "        0.70833333, 0.72164948, 0.66666667, 0.70103093, 0.72164948,\n",
       "        0.70103093, 0.69387755, 0.6875    , 0.66666667, 0.70103093,\n",
       "        0.68041237, 0.68686869, 0.71428571, 0.70103093, 0.6875    ,\n",
       "        0.66666667, 0.6875    , 0.66666667, 0.70103093, 0.70707071,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.6875    , 0.66666667,\n",
       "        0.70103093, 0.66666667, 0.70103093, 0.71287129, 0.68041237,\n",
       "        0.70103093, 0.69387755, 0.6875    , 0.69387755, 0.71428571,\n",
       "        0.65979381, 0.72727273, 0.71428571, 0.67346939, 0.71428571,\n",
       "        0.70103093, 0.68041237, 0.71428571, 0.71428571, 0.64583333,\n",
       "        0.6875    , 0.70833333, 0.64583333, 0.69387755, 0.70103093,\n",
       "        0.66666667, 0.70103093, 0.70103093, 0.64583333, 0.68041237,\n",
       "        0.70103093, 0.64583333, 0.69387755, 0.71428571, 0.66666667,\n",
       "        0.68041237, 0.68041237, 0.65979381, 0.67346939, 0.70103093,\n",
       "        0.64583333, 0.69387755, 0.66666667, 0.68041237, 0.68041237,\n",
       "        0.69387755, 0.64583333, 0.69387755, 0.71428571, 0.64583333,\n",
       "        0.70103093, 0.69387755, 0.68041237, 0.68041237, 0.69387755,\n",
       "        0.59340659, 0.65979381, 0.65979381, 0.59340659, 0.66666667,\n",
       "        0.6875    , 0.6       , 0.67346939, 0.68041237, 0.61702128,\n",
       "        0.64583333, 0.65979381, 0.60215054, 0.64583333, 0.67346939,\n",
       "        0.62365591, 0.65979381, 0.70103093, 0.61702128, 0.64583333,\n",
       "        0.67346939, 0.63157895, 0.64583333, 0.68686869, 0.65263158,\n",
       "        0.67368421, 0.70103093, 0.63157895, 0.65979381, 0.68      ,\n",
       "        0.63157895, 0.65979381, 0.69387755, 0.65263158, 0.6875    ,\n",
       "        0.70103093, 0.63157895, 0.65979381, 0.68      , 0.63829787,\n",
       "        0.66666667, 0.69387755, 0.65263158, 0.6875    , 0.6875    ]),\n",
       " 'split2_test_score': array([0.77669903, 0.75      , 0.74285714, 0.75      , 0.75      ,\n",
       "        0.75      , 0.76923077, 0.75728155, 0.76923077, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.76470588, 0.75      , 0.75      ,\n",
       "        0.74509804, 0.76470588, 0.74074074, 0.74509804, 0.73584906,\n",
       "        0.74285714, 0.76923077, 0.74285714, 0.75      , 0.75728155,\n",
       "        0.74509804, 0.74074074, 0.73786408, 0.73584906, 0.74285714,\n",
       "        0.75      , 0.75      , 0.74285714, 0.74509804, 0.75      ,\n",
       "        0.73394495, 0.72897196, 0.72897196, 0.72897196, 0.76923077,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75      , 0.73394495,\n",
       "        0.77227723, 0.7254902 , 0.74285714, 0.76470588, 0.76470588,\n",
       "        0.75728155, 0.77669903, 0.75      , 0.75728155, 0.75728155,\n",
       "        0.74285714, 0.73584906, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.74285714, 0.75728155, 0.75      , 0.72897196, 0.75728155,\n",
       "        0.75728155, 0.73786408, 0.7254902 , 0.73584906, 0.74285714,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74074074, 0.73076923, 0.72897196, 0.72897196, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73786408, 0.75      , 0.74074074,\n",
       "        0.77227723, 0.73076923, 0.73584906, 0.77227723, 0.73076923,\n",
       "        0.75      , 0.75247525, 0.75728155, 0.75      , 0.77227723,\n",
       "        0.75247525, 0.73076923, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75247525, 0.74509804, 0.74509804, 0.74509804, 0.75      ,\n",
       "        0.74285714, 0.73786408, 0.74509804, 0.74285714, 0.75      ,\n",
       "        0.74509804, 0.75728155, 0.71153846, 0.72380952, 0.73584906,\n",
       "        0.73786408, 0.73786408, 0.74285714, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.72380952, 0.72222222, 0.73584906, 0.73786408,\n",
       "        0.75      , 0.74285714, 0.75      , 0.75728155, 0.76190476,\n",
       "        0.76      , 0.76470588, 0.75471698, 0.76      , 0.76470588,\n",
       "        0.75      , 0.74747475, 0.75247525, 0.74509804, 0.75247525,\n",
       "        0.75728155, 0.77669903, 0.76470588, 0.75728155, 0.75728155,\n",
       "        0.73786408, 0.74509804, 0.74509804, 0.73786408, 0.73786408,\n",
       "        0.74285714, 0.73786408, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.74509804, 0.74509804, 0.73786408, 0.73076923, 0.72380952,\n",
       "        0.73786408, 0.74509804, 0.73786408, 0.73786408, 0.74509804,\n",
       "        0.73786408, 0.72380952, 0.71698113, 0.72897196, 0.72380952,\n",
       "        0.73076923, 0.75      , 0.72380952, 0.74509804, 0.75      ,\n",
       "        0.73469388, 0.74747475, 0.76      , 0.73469388, 0.74747475,\n",
       "        0.76      , 0.73469388, 0.76      , 0.77227723, 0.74747475,\n",
       "        0.76      , 0.76      , 0.74747475, 0.76      , 0.76470588,\n",
       "        0.74747475, 0.75      , 0.75728155, 0.74      , 0.74509804,\n",
       "        0.75728155, 0.76      , 0.74509804, 0.73786408, 0.74509804,\n",
       "        0.75      , 0.75      , 0.74      , 0.73786408, 0.72380952,\n",
       "        0.76767677, 0.74509804, 0.73786408, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.74      , 0.73076923, 0.73786408, 0.76      ,\n",
       "        0.73076923, 0.73786408, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.77669903, 0.75      , 0.74285714, 0.75      , 0.75      ,\n",
       "        0.75      , 0.76923077, 0.75728155, 0.76923077, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.76470588, 0.75      , 0.75      ,\n",
       "        0.74509804, 0.76470588, 0.74074074, 0.74509804, 0.73584906,\n",
       "        0.74285714, 0.76923077, 0.74285714, 0.75      , 0.75728155,\n",
       "        0.74509804, 0.74074074, 0.73786408, 0.73584906, 0.74285714,\n",
       "        0.75      , 0.75      , 0.74285714, 0.74509804, 0.75      ,\n",
       "        0.73394495, 0.72897196, 0.72897196, 0.72897196, 0.76923077,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75      , 0.73394495,\n",
       "        0.77227723, 0.7254902 , 0.74285714, 0.76470588, 0.76470588,\n",
       "        0.75728155, 0.77669903, 0.75      , 0.75728155, 0.75728155,\n",
       "        0.74285714, 0.73584906, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.74285714, 0.75728155, 0.75      , 0.72897196, 0.75728155,\n",
       "        0.75728155, 0.73786408, 0.7254902 , 0.73584906, 0.74285714,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74074074, 0.73076923, 0.72897196, 0.72897196, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73786408, 0.75      , 0.74074074,\n",
       "        0.77227723, 0.73076923, 0.73584906, 0.77227723, 0.73076923,\n",
       "        0.75      , 0.75247525, 0.75728155, 0.75      , 0.77227723,\n",
       "        0.75247525, 0.73076923, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75247525, 0.74509804, 0.74509804, 0.74509804, 0.75      ,\n",
       "        0.74285714, 0.73786408, 0.74509804, 0.74285714, 0.75      ,\n",
       "        0.74509804, 0.75728155, 0.71153846, 0.72380952, 0.73584906,\n",
       "        0.73786408, 0.73786408, 0.74285714, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.72380952, 0.72222222, 0.73584906, 0.73786408,\n",
       "        0.75      , 0.74285714, 0.75      , 0.75728155, 0.76190476,\n",
       "        0.76      , 0.76470588, 0.75471698, 0.76      , 0.76470588,\n",
       "        0.75      , 0.74747475, 0.75247525, 0.74509804, 0.75247525,\n",
       "        0.75728155, 0.77669903, 0.76470588, 0.75728155, 0.75728155,\n",
       "        0.73786408, 0.74509804, 0.74509804, 0.73786408, 0.73786408,\n",
       "        0.74285714, 0.73786408, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.74509804, 0.74509804, 0.73786408, 0.73076923, 0.72380952,\n",
       "        0.73786408, 0.74509804, 0.73786408, 0.73786408, 0.74509804,\n",
       "        0.73786408, 0.72380952, 0.71698113, 0.72897196, 0.72380952,\n",
       "        0.73076923, 0.75      , 0.72380952, 0.74509804, 0.75      ,\n",
       "        0.73469388, 0.74747475, 0.76      , 0.73469388, 0.74747475,\n",
       "        0.76      , 0.73469388, 0.76      , 0.77227723, 0.74747475,\n",
       "        0.76      , 0.76      , 0.74747475, 0.76      , 0.76470588,\n",
       "        0.74747475, 0.75      , 0.75728155, 0.74      , 0.74509804,\n",
       "        0.75728155, 0.76      , 0.74509804, 0.73786408, 0.74509804,\n",
       "        0.75      , 0.75      , 0.74      , 0.73786408, 0.72380952,\n",
       "        0.76767677, 0.74509804, 0.73786408, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.74      , 0.73076923, 0.73786408, 0.76      ,\n",
       "        0.73076923, 0.73786408, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.76923077, 0.71153846, 0.72222222, 0.76190476, 0.75728155,\n",
       "        0.75      , 0.75      , 0.75      , 0.75728155, 0.75728155,\n",
       "        0.7184466 , 0.73584906, 0.74285714, 0.75728155, 0.73584906,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.73584906, 0.74074074,\n",
       "        0.72222222, 0.75      , 0.74285714, 0.73584906, 0.75      ,\n",
       "        0.77358491, 0.76190476, 0.73786408, 0.71698113, 0.71559633,\n",
       "        0.74285714, 0.74285714, 0.74285714, 0.75      , 0.75      ,\n",
       "        0.74766355, 0.75471698, 0.72897196, 0.72897196, 0.73076923,\n",
       "        0.72897196, 0.72897196, 0.73786408, 0.74285714, 0.73394495,\n",
       "        0.77227723, 0.74074074, 0.74285714, 0.76      , 0.75471698,\n",
       "        0.74285714, 0.75      , 0.75      , 0.75728155, 0.73786408,\n",
       "        0.73076923, 0.74074074, 0.75      , 0.75      , 0.75      ,\n",
       "        0.75      , 0.75      , 0.75728155, 0.74509804, 0.71698113,\n",
       "        0.71698113, 0.74285714, 0.75      , 0.74285714, 0.75728155,\n",
       "        0.75728155, 0.75      , 0.71153846, 0.74766355, 0.72897196,\n",
       "        0.73786408, 0.74285714, 0.74285714, 0.74509804, 0.75728155,\n",
       "        0.75      , 0.7184466 , 0.72897196, 0.72222222, 0.73786408,\n",
       "        0.72897196, 0.72897196, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.76      , 0.76635514, 0.75471698, 0.76      , 0.75471698,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.74285714, 0.76      ,\n",
       "        0.73267327, 0.75728155, 0.74      , 0.75      , 0.75      ,\n",
       "        0.74509804, 0.74509804, 0.76470588, 0.76470588, 0.74766355,\n",
       "        0.73584906, 0.73267327, 0.75      , 0.74285714, 0.73786408,\n",
       "        0.75728155, 0.75      , 0.70588235, 0.74285714, 0.72897196,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.74509804, 0.75728155,\n",
       "        0.74285714, 0.70588235, 0.73394495, 0.74074074, 0.73786408,\n",
       "        0.74285714, 0.73584906, 0.73786408, 0.75      , 0.75      ,\n",
       "        0.74747475, 0.78431373, 0.75471698, 0.74747475, 0.75728155,\n",
       "        0.76190476, 0.74747475, 0.76923077, 0.75728155, 0.75247525,\n",
       "        0.76923077, 0.75471698, 0.74      , 0.75      , 0.75      ,\n",
       "        0.75247525, 0.76470588, 0.76923077, 0.73267327, 0.74766355,\n",
       "        0.74285714, 0.76      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.7184466 , 0.74074074,\n",
       "        0.75247525, 0.73076923, 0.73786408, 0.74509804, 0.75      ,\n",
       "        0.75      , 0.75247525, 0.73786408, 0.74766355, 0.75247525,\n",
       "        0.73786408, 0.74285714, 0.73786408, 0.75      , 0.76190476,\n",
       "        0.71578947, 0.74747475, 0.76      , 0.72340426, 0.74747475,\n",
       "        0.75247525, 0.7628866 , 0.74      , 0.77227723, 0.74747475,\n",
       "        0.76470588, 0.76470588, 0.74226804, 0.75247525, 0.75247525,\n",
       "        0.75510204, 0.75247525, 0.75728155, 0.76767677, 0.76470588,\n",
       "        0.75728155, 0.76      , 0.75247525, 0.73786408, 0.74747475,\n",
       "        0.74509804, 0.75      , 0.75510204, 0.75      , 0.73584906,\n",
       "        0.76      , 0.75247525, 0.74509804, 0.74509804, 0.74509804,\n",
       "        0.75728155, 0.74747475, 0.75728155, 0.73584906, 0.74747475,\n",
       "        0.75247525, 0.73076923, 0.75247525, 0.74509804, 0.75728155]),\n",
       " 'split3_test_score': array([0.8       , 0.78846154, 0.77358491, 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.8       , 0.78846154, 0.7961165 , 0.8       ,\n",
       "        0.77669903, 0.77358491, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.78846154, 0.7961165 , 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.76190476, 0.8       , 0.76190476, 0.76190476, 0.7961165 ,\n",
       "        0.78846154, 0.78095238, 0.78095238, 0.76923077, 0.76923077,\n",
       "        0.8       , 0.77669903, 0.78095238, 0.7961165 , 0.78846154,\n",
       "        0.78504673, 0.76923077, 0.76923077, 0.76923077, 0.78095238,\n",
       "        0.75728155, 0.76923077, 0.80769231, 0.78846154, 0.79245283,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.78095238, 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.8       , 0.8       ,\n",
       "        0.77358491, 0.77358491, 0.79245283, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.78846154, 0.76190476,\n",
       "        0.77358491, 0.80769231, 0.76923077, 0.77358491, 0.8       ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.75471698, 0.76190476,\n",
       "        0.80769231, 0.77358491, 0.78095238, 0.80769231, 0.78846154,\n",
       "        0.8       , 0.8       , 0.76190476, 0.76923077, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78846154, 0.76923077, 0.79245283,\n",
       "        0.78846154, 0.76923077, 0.8       , 0.8       , 0.78095238,\n",
       "        0.78846154, 0.77358491, 0.78095238, 0.78846154, 0.78846154,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.8       , 0.79245283,\n",
       "        0.77358491, 0.7961165 , 0.78846154, 0.76923077, 0.7961165 ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76190476, 0.75471698,\n",
       "        0.80769231, 0.77358491, 0.76923077, 0.80769231, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.80769231, 0.7961165 , 0.7961165 ,\n",
       "        0.76190476, 0.78846154, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.77227723, 0.78095238, 0.79245283, 0.77669903,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.78095238, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.78846154, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.80769231, 0.7961165 , 0.76190476,\n",
       "        0.80769231, 0.8       , 0.77358491, 0.7961165 , 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76190476, 0.80769231,\n",
       "        0.78504673, 0.77358491, 0.7961165 , 0.80769231, 0.7961165 ,\n",
       "        0.75510204, 0.75728155, 0.76923077, 0.75510204, 0.75728155,\n",
       "        0.76190476, 0.73267327, 0.75728155, 0.76190476, 0.75247525,\n",
       "        0.75728155, 0.78846154, 0.75728155, 0.75728155, 0.78095238,\n",
       "        0.73786408, 0.75728155, 0.76923077, 0.7961165 , 0.77669903,\n",
       "        0.7961165 , 0.7961165 , 0.78846154, 0.78846154, 0.75471698,\n",
       "        0.78095238, 0.78846154, 0.77227723, 0.77669903, 0.7961165 ,\n",
       "        0.77227723, 0.77669903, 0.8       , 0.76923077, 0.78846154,\n",
       "        0.8       , 0.7961165 , 0.78846154, 0.78846154, 0.78431373,\n",
       "        0.78846154, 0.8       , 0.76923077, 0.78846154, 0.80769231,\n",
       "        0.8       , 0.78846154, 0.77358491, 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.8       , 0.78846154, 0.7961165 , 0.8       ,\n",
       "        0.77669903, 0.77358491, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.78846154, 0.7961165 , 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.76190476, 0.8       , 0.76190476, 0.76190476, 0.7961165 ,\n",
       "        0.78846154, 0.78095238, 0.78095238, 0.76923077, 0.76923077,\n",
       "        0.8       , 0.77669903, 0.78095238, 0.7961165 , 0.78846154,\n",
       "        0.78504673, 0.76923077, 0.76923077, 0.76923077, 0.78095238,\n",
       "        0.75728155, 0.76923077, 0.80769231, 0.78846154, 0.79245283,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.78095238, 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.8       , 0.8       ,\n",
       "        0.77358491, 0.77358491, 0.79245283, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.78846154, 0.76190476,\n",
       "        0.77358491, 0.80769231, 0.76923077, 0.77358491, 0.8       ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.75471698, 0.76190476,\n",
       "        0.80769231, 0.77358491, 0.78095238, 0.80769231, 0.78846154,\n",
       "        0.8       , 0.8       , 0.76190476, 0.76923077, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78846154, 0.76923077, 0.79245283,\n",
       "        0.78846154, 0.76923077, 0.8       , 0.8       , 0.78095238,\n",
       "        0.78846154, 0.77358491, 0.78095238, 0.78846154, 0.78846154,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.8       , 0.79245283,\n",
       "        0.77358491, 0.7961165 , 0.78846154, 0.76923077, 0.7961165 ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76190476, 0.75471698,\n",
       "        0.80769231, 0.77358491, 0.76923077, 0.80769231, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.80769231, 0.7961165 , 0.7961165 ,\n",
       "        0.76190476, 0.78846154, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.77227723, 0.78095238, 0.79245283, 0.77669903,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.78095238, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.78846154, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.80769231, 0.7961165 , 0.76190476,\n",
       "        0.80769231, 0.8       , 0.77358491, 0.7961165 , 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76190476, 0.80769231,\n",
       "        0.78504673, 0.77358491, 0.7961165 , 0.80769231, 0.7961165 ,\n",
       "        0.75510204, 0.75728155, 0.76923077, 0.75510204, 0.75728155,\n",
       "        0.76190476, 0.73267327, 0.75728155, 0.76190476, 0.75247525,\n",
       "        0.75728155, 0.78846154, 0.75728155, 0.75728155, 0.78095238,\n",
       "        0.73786408, 0.75728155, 0.76923077, 0.7961165 , 0.77669903,\n",
       "        0.7961165 , 0.7961165 , 0.78846154, 0.78846154, 0.75471698,\n",
       "        0.78095238, 0.78846154, 0.77227723, 0.77669903, 0.7961165 ,\n",
       "        0.77227723, 0.77669903, 0.8       , 0.76923077, 0.78846154,\n",
       "        0.8       , 0.7961165 , 0.78846154, 0.78846154, 0.78431373,\n",
       "        0.78846154, 0.8       , 0.76923077, 0.78846154, 0.80769231,\n",
       "        0.8       , 0.78504673, 0.79245283, 0.8       , 0.79245283,\n",
       "        0.8       , 0.81132075, 0.78846154, 0.81132075, 0.8       ,\n",
       "        0.79245283, 0.78095238, 0.80373832, 0.78846154, 0.78095238,\n",
       "        0.81904762, 0.80769231, 0.80769231, 0.81904762, 0.77358491,\n",
       "        0.77358491, 0.81132075, 0.78095238, 0.77358491, 0.83018868,\n",
       "        0.78846154, 0.7961165 , 0.80769231, 0.76923077, 0.78095238,\n",
       "        0.81904762, 0.78095238, 0.78095238, 0.81904762, 0.80769231,\n",
       "        0.8       , 0.8       , 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.78095238, 0.77358491, 0.81904762, 0.8       , 0.8       ,\n",
       "        0.78846154, 0.8       , 0.78504673, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.79245283, 0.78846154, 0.81132075,\n",
       "        0.78095238, 0.78846154, 0.8       , 0.78846154, 0.78095238,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.82242991, 0.78095238,\n",
       "        0.77358491, 0.80769231, 0.7961165 , 0.77358491, 0.81132075,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76923077, 0.77358491,\n",
       "        0.81904762, 0.78504673, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.78846154, 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.79245283, 0.77358491, 0.81904762, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.76470588, 0.79245283, 0.7961165 , 0.76470588,\n",
       "        0.8       , 0.79245283, 0.77669903, 0.79245283, 0.77669903,\n",
       "        0.77669903, 0.81904762, 0.80769231, 0.7961165 , 0.80769231,\n",
       "        0.77358491, 0.78095238, 0.8       , 0.77358491, 0.78095238,\n",
       "        0.81904762, 0.7961165 , 0.80769231, 0.78095238, 0.77358491,\n",
       "        0.80769231, 0.79245283, 0.79245283, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.76923077, 0.77669903, 0.80769231,\n",
       "        0.79245283, 0.78095238, 0.80769231, 0.81904762, 0.79245283,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.77227723, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.79245283, 0.76470588,\n",
       "        0.80373832, 0.79245283, 0.76470588, 0.79245283, 0.79245283,\n",
       "        0.76470588, 0.80373832, 0.81904762, 0.77669903, 0.80373832,\n",
       "        0.79245283, 0.76470588, 0.79245283, 0.79245283, 0.76923077,\n",
       "        0.79245283, 0.80769231, 0.77227723, 0.8       , 0.79245283,\n",
       "        0.77227723, 0.78846154, 0.79245283, 0.76470588, 0.8       ,\n",
       "        0.81904762, 0.78846154, 0.8       , 0.79245283, 0.78846154,\n",
       "        0.8       , 0.78504673, 0.78846154, 0.81132075, 0.81904762,\n",
       "        0.77083333, 0.78      , 0.75728155, 0.75510204, 0.78      ,\n",
       "        0.75728155, 0.7628866 , 0.78      , 0.75728155, 0.76767677,\n",
       "        0.77227723, 0.75728155, 0.75510204, 0.77227723, 0.75728155,\n",
       "        0.74226804, 0.77227723, 0.75728155, 0.78      , 0.77227723,\n",
       "        0.78095238, 0.78      , 0.76470588, 0.78095238, 0.76767677,\n",
       "        0.76      , 0.78095238, 0.78350515, 0.77227723, 0.78846154,\n",
       "        0.75510204, 0.76470588, 0.78095238, 0.75510204, 0.76      ,\n",
       "        0.78095238, 0.79591837, 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.79245283, 0.75510204, 0.77227723, 0.78846154]),\n",
       " 'split4_test_score': array([0.76470588, 0.80769231, 0.8       , 0.74      , 0.76767677,\n",
       "        0.78      , 0.71428571, 0.75510204, 0.7755102 , 0.78787879,\n",
       "        0.80412371, 0.8       , 0.76      , 0.75510204, 0.76767677,\n",
       "        0.73267327, 0.75789474, 0.76767677, 0.82      , 0.82828283,\n",
       "        0.79591837, 0.78431373, 0.78787879, 0.77227723, 0.76      ,\n",
       "        0.75510204, 0.76      , 0.78787879, 0.80808081, 0.76767677,\n",
       "        0.78787879, 0.78787879, 0.78      , 0.76      , 0.75510204,\n",
       "        0.77227723, 0.80392157, 0.80808081, 0.79207921, 0.79207921,\n",
       "        0.78787879, 0.78      , 0.78      , 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.78846154, 0.80808081, 0.74509804, 0.76      ,\n",
       "        0.79207921, 0.74      , 0.74      , 0.7628866 , 0.76767677,\n",
       "        0.8125    , 0.7755102 , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.72727273, 0.74226804, 0.7628866 , 0.78787879, 0.82      ,\n",
       "        0.8       , 0.76767677, 0.78350515, 0.78787879, 0.75510204,\n",
       "        0.7755102 , 0.78      , 0.78787879, 0.82      , 0.8       ,\n",
       "        0.79207921, 0.78787879, 0.7755102 , 0.76      , 0.7628866 ,\n",
       "        0.76767677, 0.8       , 0.83168317, 0.8       , 0.79207921,\n",
       "        0.80392157, 0.79207921, 0.77227723, 0.75510204, 0.76      ,\n",
       "        0.74226804, 0.80392157, 0.80392157, 0.73469388, 0.7961165 ,\n",
       "        0.76      , 0.73469388, 0.73267327, 0.74747475, 0.76      ,\n",
       "        0.77227723, 0.79591837, 0.72727273, 0.7755102 , 0.75510204,\n",
       "        0.74509804, 0.72727273, 0.7628866 , 0.78      , 0.79591837,\n",
       "        0.80808081, 0.74747475, 0.76      , 0.79591837, 0.76      ,\n",
       "        0.74747475, 0.7628866 , 0.8       , 0.83168317, 0.80808081,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.76      , 0.75510204,\n",
       "        0.7628866 , 0.80392157, 0.82352941, 0.82      , 0.81188119,\n",
       "        0.78431373, 0.79207921, 0.77227723, 0.7628866 , 0.7628866 ,\n",
       "        0.74226804, 0.75247525, 0.80392157, 0.74226804, 0.7254902 ,\n",
       "        0.76      , 0.72164948, 0.73267327, 0.72340426, 0.75510204,\n",
       "        0.79207921, 0.78787879, 0.74      , 0.77669903, 0.77227723,\n",
       "        0.74509804, 0.73267327, 0.72727273, 0.76767677, 0.79207921,\n",
       "        0.82828283, 0.75510204, 0.78431373, 0.79591837, 0.76767677,\n",
       "        0.74      , 0.75      , 0.8       , 0.80392157, 0.82352941,\n",
       "        0.76767677, 0.7961165 , 0.80392157, 0.74747475, 0.76      ,\n",
       "        0.74226804, 0.8       , 0.8       , 0.83168317, 0.78787879,\n",
       "        0.81553398, 0.80392157, 0.7755102 , 0.78      , 0.7628866 ,\n",
       "        0.73913043, 0.75      , 0.74226804, 0.73913043, 0.75789474,\n",
       "        0.74226804, 0.7311828 , 0.73684211, 0.73469388, 0.73913043,\n",
       "        0.75      , 0.7628866 , 0.73913043, 0.74226804, 0.75510204,\n",
       "        0.7311828 , 0.75510204, 0.76      , 0.77083333, 0.78350515,\n",
       "        0.7755102 , 0.73684211, 0.7755102 , 0.77227723, 0.70967742,\n",
       "        0.74747475, 0.77227723, 0.77894737, 0.78      , 0.80392157,\n",
       "        0.77083333, 0.7755102 , 0.77227723, 0.7311828 , 0.75510204,\n",
       "        0.76      , 0.78723404, 0.77227723, 0.78846154, 0.77894737,\n",
       "        0.78      , 0.80392157, 0.71578947, 0.75510204, 0.78      ,\n",
       "        0.76470588, 0.80769231, 0.8       , 0.74      , 0.76767677,\n",
       "        0.78      , 0.71428571, 0.75510204, 0.7755102 , 0.78787879,\n",
       "        0.80412371, 0.8       , 0.76      , 0.75510204, 0.76767677,\n",
       "        0.73267327, 0.75789474, 0.76767677, 0.82      , 0.82828283,\n",
       "        0.79591837, 0.78431373, 0.78787879, 0.77227723, 0.76      ,\n",
       "        0.75510204, 0.76      , 0.78787879, 0.80808081, 0.76767677,\n",
       "        0.78787879, 0.78787879, 0.78      , 0.76      , 0.75510204,\n",
       "        0.77227723, 0.80392157, 0.80808081, 0.79207921, 0.79207921,\n",
       "        0.78787879, 0.78      , 0.78      , 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.78846154, 0.80808081, 0.74509804, 0.76      ,\n",
       "        0.79207921, 0.74      , 0.74      , 0.7628866 , 0.76767677,\n",
       "        0.8125    , 0.7755102 , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.72727273, 0.74226804, 0.7628866 , 0.78787879, 0.82      ,\n",
       "        0.8       , 0.76767677, 0.78350515, 0.78787879, 0.75510204,\n",
       "        0.7755102 , 0.78      , 0.78787879, 0.82      , 0.8       ,\n",
       "        0.79207921, 0.78787879, 0.7755102 , 0.76      , 0.7628866 ,\n",
       "        0.76767677, 0.8       , 0.83168317, 0.8       , 0.79207921,\n",
       "        0.80392157, 0.79207921, 0.77227723, 0.75510204, 0.76      ,\n",
       "        0.74226804, 0.80392157, 0.80392157, 0.73469388, 0.7961165 ,\n",
       "        0.76      , 0.73469388, 0.73267327, 0.74747475, 0.76      ,\n",
       "        0.77227723, 0.79591837, 0.72727273, 0.7755102 , 0.75510204,\n",
       "        0.74509804, 0.72727273, 0.7628866 , 0.78      , 0.79591837,\n",
       "        0.80808081, 0.74747475, 0.76      , 0.79591837, 0.76      ,\n",
       "        0.74747475, 0.7628866 , 0.8       , 0.83168317, 0.80808081,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.76      , 0.75510204,\n",
       "        0.7628866 , 0.80392157, 0.82352941, 0.82      , 0.81188119,\n",
       "        0.78431373, 0.79207921, 0.77227723, 0.7628866 , 0.7628866 ,\n",
       "        0.74226804, 0.75247525, 0.80392157, 0.74226804, 0.7254902 ,\n",
       "        0.76      , 0.72164948, 0.73267327, 0.72340426, 0.75510204,\n",
       "        0.79207921, 0.78787879, 0.74      , 0.77669903, 0.77227723,\n",
       "        0.74509804, 0.73267327, 0.72727273, 0.76767677, 0.79207921,\n",
       "        0.82828283, 0.75510204, 0.78431373, 0.79591837, 0.76767677,\n",
       "        0.74      , 0.75      , 0.8       , 0.80392157, 0.82352941,\n",
       "        0.76767677, 0.7961165 , 0.80392157, 0.74747475, 0.76      ,\n",
       "        0.74226804, 0.8       , 0.8       , 0.83168317, 0.78787879,\n",
       "        0.81553398, 0.80392157, 0.7755102 , 0.78      , 0.7628866 ,\n",
       "        0.73913043, 0.75      , 0.74226804, 0.73913043, 0.75789474,\n",
       "        0.74226804, 0.7311828 , 0.73684211, 0.73469388, 0.73913043,\n",
       "        0.75      , 0.7628866 , 0.73913043, 0.74226804, 0.75510204,\n",
       "        0.7311828 , 0.75510204, 0.76      , 0.77083333, 0.78350515,\n",
       "        0.7755102 , 0.73684211, 0.7755102 , 0.77227723, 0.70967742,\n",
       "        0.74747475, 0.77227723, 0.77894737, 0.78      , 0.80392157,\n",
       "        0.77083333, 0.7755102 , 0.77227723, 0.7311828 , 0.75510204,\n",
       "        0.76      , 0.78723404, 0.77227723, 0.78846154, 0.77894737,\n",
       "        0.78      , 0.80392157, 0.71578947, 0.75510204, 0.78      ,\n",
       "        0.76      , 0.80392157, 0.8       , 0.74509804, 0.77227723,\n",
       "        0.76767677, 0.74509804, 0.73267327, 0.74747475, 0.78      ,\n",
       "        0.80412371, 0.81632653, 0.74      , 0.77227723, 0.78      ,\n",
       "        0.74      , 0.74747475, 0.74747475, 0.78431373, 0.80808081,\n",
       "        0.7755102 , 0.75247525, 0.76470588, 0.77227723, 0.74509804,\n",
       "        0.7755102 , 0.77227723, 0.78      , 0.79591837, 0.7755102 ,\n",
       "        0.76470588, 0.77227723, 0.77669903, 0.76      , 0.77227723,\n",
       "        0.78431373, 0.79207921, 0.79591837, 0.78787879, 0.78      ,\n",
       "        0.77227723, 0.77227723, 0.75247525, 0.74747475, 0.77227723,\n",
       "        0.74747475, 0.7961165 , 0.83168317, 0.73267327, 0.77669903,\n",
       "        0.74509804, 0.74      , 0.73786408, 0.75247525, 0.74747475,\n",
       "        0.8       , 0.80808081, 0.76      , 0.77227723, 0.76      ,\n",
       "        0.74747475, 0.7254902 , 0.73469388, 0.76470588, 0.8       ,\n",
       "        0.81632653, 0.74      , 0.76470588, 0.76470588, 0.74747475,\n",
       "        0.75247525, 0.77227723, 0.7961165 , 0.77227723, 0.79591837,\n",
       "        0.78431373, 0.76470588, 0.77227723, 0.76470588, 0.75728155,\n",
       "        0.78      , 0.80769231, 0.81188119, 0.78787879, 0.78431373,\n",
       "        0.76470588, 0.79207921, 0.78      , 0.77227723, 0.78      ,\n",
       "        0.75510204, 0.78846154, 0.7961165 , 0.75510204, 0.78095238,\n",
       "        0.75728155, 0.74226804, 0.7254902 , 0.73267327, 0.73469388,\n",
       "        0.77669903, 0.79591837, 0.72727273, 0.7961165 , 0.76      ,\n",
       "        0.76      , 0.73076923, 0.74      , 0.73267327, 0.77669903,\n",
       "        0.82474227, 0.74747475, 0.74509804, 0.77227723, 0.73469388,\n",
       "        0.75      , 0.74747475, 0.76470588, 0.80392157, 0.8       ,\n",
       "        0.74747475, 0.75728155, 0.78      , 0.74747475, 0.74509804,\n",
       "        0.74      , 0.78846154, 0.77227723, 0.8       , 0.77227723,\n",
       "        0.76470588, 0.77227723, 0.76      , 0.74509804, 0.75247525,\n",
       "        0.73333333, 0.76      , 0.78846154, 0.73913043, 0.74509804,\n",
       "        0.8       , 0.7311828 , 0.73267327, 0.7254902 , 0.74468085,\n",
       "        0.75728155, 0.77669903, 0.73684211, 0.73786408, 0.77669903,\n",
       "        0.72164948, 0.7184466 , 0.73076923, 0.74226804, 0.78095238,\n",
       "        0.79207921, 0.73469388, 0.76190476, 0.77669903, 0.72916667,\n",
       "        0.73786408, 0.75      , 0.74226804, 0.77669903, 0.81188119,\n",
       "        0.73469388, 0.75471698, 0.76923077, 0.72164948, 0.73786408,\n",
       "        0.75247525, 0.74226804, 0.76190476, 0.80392157, 0.75510204,\n",
       "        0.77358491, 0.76923077, 0.74226804, 0.73786408, 0.75247525,\n",
       "        0.68965517, 0.74725275, 0.72916667, 0.6744186 , 0.73913043,\n",
       "        0.72727273, 0.66666667, 0.72916667, 0.72727273, 0.68817204,\n",
       "        0.73469388, 0.74      , 0.70212766, 0.73469388, 0.74      ,\n",
       "        0.65217391, 0.74226804, 0.73267327, 0.69565217, 0.74747475,\n",
       "        0.74      , 0.70212766, 0.74747475, 0.74509804, 0.67391304,\n",
       "        0.74      , 0.75247525, 0.70967742, 0.74747475, 0.75247525,\n",
       "        0.7173913 , 0.73469388, 0.74509804, 0.67391304, 0.74      ,\n",
       "        0.76      , 0.70967742, 0.74747475, 0.77669903, 0.7032967 ,\n",
       "        0.74226804, 0.76470588, 0.68131868, 0.72727273, 0.75247525]),\n",
       " 'mean_test_score': array([0.78155639, 0.78443445, 0.77583743, 0.77111749, 0.77765161,\n",
       "        0.77907359, 0.77358221, 0.77445474, 0.78818719, 0.77290935,\n",
       "        0.77274226, 0.76460172, 0.77840962, 0.770205  , 0.76987138,\n",
       "        0.76563523, 0.78100533, 0.77800326, 0.7837393 , 0.77965078,\n",
       "        0.76237049, 0.78391369, 0.76817744, 0.76640095, 0.78498353,\n",
       "        0.77722012, 0.77532892, 0.77223649, 0.77219668, 0.75890046,\n",
       "        0.77486162, 0.78346458, 0.77285024, 0.77764973, 0.77804011,\n",
       "        0.78041659, 0.77562854, 0.7662427 , 0.75786221, 0.77638391,\n",
       "        0.77501194, 0.76972595, 0.78541872, 0.78100975, 0.77934056,\n",
       "        0.76674167, 0.7700608 , 0.77685426, 0.78138358, 0.78021883,\n",
       "        0.78349853, 0.78053291, 0.77691026, 0.78144045, 0.77175383,\n",
       "        0.77832886, 0.76499511, 0.76973222, 0.77538507, 0.76217687,\n",
       "        0.76646635, 0.77767684, 0.78106949, 0.76962101, 0.77916807,\n",
       "        0.78036533, 0.76874205, 0.7708413 , 0.77296092, 0.77818363,\n",
       "        0.78747771, 0.78807395, 0.77009461, 0.77786478, 0.76911914,\n",
       "        0.78145069, 0.77194414, 0.77873773, 0.78070169, 0.77665362,\n",
       "        0.78133136, 0.7720362 , 0.77859398, 0.76713939, 0.77563795,\n",
       "        0.78187455, 0.78679402, 0.7775984 , 0.7795711 , 0.78173769,\n",
       "        0.75953072, 0.77628281, 0.78197476, 0.76691721, 0.7780834 ,\n",
       "        0.78226418, 0.76530685, 0.77580353, 0.77542902, 0.76396953,\n",
       "        0.78155306, 0.77746132, 0.75714424, 0.77200192, 0.77447118,\n",
       "        0.75889737, 0.77243656, 0.77391547, 0.77079905, 0.78188993,\n",
       "        0.78398824, 0.75902791, 0.7670857 , 0.76348356, 0.77137149,\n",
       "        0.77248145, 0.77473492, 0.77079352, 0.78035841, 0.7740264 ,\n",
       "        0.77016788, 0.76839968, 0.77729137, 0.76704894, 0.77187847,\n",
       "        0.78272414, 0.77355192, 0.77496018, 0.77678153, 0.77853151,\n",
       "        0.76975786, 0.78180617, 0.77193157, 0.77704802, 0.78397355,\n",
       "        0.76015779, 0.77345843, 0.77923837, 0.76072616, 0.76998127,\n",
       "        0.77470625, 0.7572939 , 0.76842386, 0.76308846, 0.75797393,\n",
       "        0.77515615, 0.78332173, 0.75810776, 0.77182023, 0.78161867,\n",
       "        0.76035776, 0.7643879 , 0.77243656, 0.76662578, 0.77167913,\n",
       "        0.78446075, 0.75963064, 0.77636275, 0.78403921, 0.76711869,\n",
       "        0.77277376, 0.76881983, 0.77093806, 0.77765785, 0.77334513,\n",
       "        0.76538347, 0.77773703, 0.77949806, 0.7636143 , 0.77306128,\n",
       "        0.76582665, 0.77158437, 0.7688311 , 0.78430396, 0.76520946,\n",
       "        0.77576411, 0.77555031, 0.76729343, 0.77763804, 0.77162178,\n",
       "        0.74477841, 0.75163153, 0.76420292, 0.74332913, 0.75179204,\n",
       "        0.76538867, 0.73474545, 0.75425414, 0.75693174, 0.73973098,\n",
       "        0.75554009, 0.7694666 , 0.74351419, 0.75117175, 0.77005522,\n",
       "        0.73663766, 0.75178242, 0.76550865, 0.75642543, 0.76448986,\n",
       "        0.77425413, 0.75613558, 0.76901093, 0.77227468, 0.7368453 ,\n",
       "        0.75900675, 0.77035394, 0.75719229, 0.7636333 , 0.77089197,\n",
       "        0.76632413, 0.75845135, 0.76752826, 0.75250247, 0.76342264,\n",
       "        0.76812745, 0.76607362, 0.76004064, 0.76469647, 0.76568315,\n",
       "        0.7601966 , 0.77385713, 0.74902065, 0.76203404, 0.77494142,\n",
       "        0.78155639, 0.78443445, 0.77583743, 0.77111749, 0.77765161,\n",
       "        0.77907359, 0.77358221, 0.77445474, 0.78818719, 0.77290935,\n",
       "        0.77274226, 0.76460172, 0.77840962, 0.770205  , 0.76987138,\n",
       "        0.76563523, 0.78100533, 0.77800326, 0.7837393 , 0.77965078,\n",
       "        0.76237049, 0.78391369, 0.76817744, 0.76640095, 0.78498353,\n",
       "        0.77722012, 0.77532892, 0.77223649, 0.77219668, 0.75890046,\n",
       "        0.77486162, 0.78346458, 0.77285024, 0.77764973, 0.77804011,\n",
       "        0.78041659, 0.77562854, 0.7662427 , 0.75786221, 0.77638391,\n",
       "        0.77501194, 0.76972595, 0.78541872, 0.78100975, 0.77934056,\n",
       "        0.76674167, 0.7700608 , 0.77685426, 0.78138358, 0.78021883,\n",
       "        0.78349853, 0.78053291, 0.77691026, 0.78144045, 0.77175383,\n",
       "        0.77832886, 0.76499511, 0.76973222, 0.77538507, 0.76217687,\n",
       "        0.76646635, 0.77767684, 0.78106949, 0.76962101, 0.77916807,\n",
       "        0.78036533, 0.76874205, 0.7708413 , 0.77296092, 0.77818363,\n",
       "        0.78747771, 0.78807395, 0.77009461, 0.77786478, 0.76911914,\n",
       "        0.78145069, 0.77194414, 0.77873773, 0.78070169, 0.77665362,\n",
       "        0.78133136, 0.7720362 , 0.77859398, 0.76713939, 0.77563795,\n",
       "        0.78187455, 0.78679402, 0.7775984 , 0.7795711 , 0.78173769,\n",
       "        0.75953072, 0.77628281, 0.78197476, 0.76691721, 0.7780834 ,\n",
       "        0.78226418, 0.76530685, 0.77580353, 0.77542902, 0.76396953,\n",
       "        0.78155306, 0.77746132, 0.75714424, 0.77200192, 0.77447118,\n",
       "        0.75889737, 0.77243656, 0.77391547, 0.77079905, 0.78188993,\n",
       "        0.78398824, 0.75902791, 0.7670857 , 0.76348356, 0.77137149,\n",
       "        0.77248145, 0.77473492, 0.77079352, 0.78035841, 0.7740264 ,\n",
       "        0.77016788, 0.76839968, 0.77729137, 0.76704894, 0.77187847,\n",
       "        0.78272414, 0.77355192, 0.77496018, 0.77678153, 0.77853151,\n",
       "        0.76975786, 0.78180617, 0.77193157, 0.77704802, 0.78397355,\n",
       "        0.76015779, 0.77345843, 0.77923837, 0.76072616, 0.76998127,\n",
       "        0.77470625, 0.7572939 , 0.76842386, 0.76308846, 0.75797393,\n",
       "        0.77515615, 0.78332173, 0.75810776, 0.77182023, 0.78161867,\n",
       "        0.76035776, 0.7643879 , 0.77243656, 0.76662578, 0.77167913,\n",
       "        0.78446075, 0.75963064, 0.77636275, 0.78403921, 0.76711869,\n",
       "        0.77277376, 0.76881983, 0.77093806, 0.77765785, 0.77334513,\n",
       "        0.76538347, 0.77773703, 0.77949806, 0.7636143 , 0.77306128,\n",
       "        0.76582665, 0.77158437, 0.7688311 , 0.78430396, 0.76520946,\n",
       "        0.77576411, 0.77555031, 0.76729343, 0.77763804, 0.77162178,\n",
       "        0.74477841, 0.75163153, 0.76420292, 0.74332913, 0.75179204,\n",
       "        0.76538867, 0.73474545, 0.75425414, 0.75693174, 0.73973098,\n",
       "        0.75554009, 0.7694666 , 0.74351419, 0.75117175, 0.77005522,\n",
       "        0.73663766, 0.75178242, 0.76550865, 0.75642543, 0.76448986,\n",
       "        0.77425413, 0.75613558, 0.76901093, 0.77227468, 0.7368453 ,\n",
       "        0.75900675, 0.77035394, 0.75719229, 0.7636333 , 0.77089197,\n",
       "        0.76632413, 0.75845135, 0.76752826, 0.75250247, 0.76342264,\n",
       "        0.76812745, 0.76607362, 0.76004064, 0.76469647, 0.76568315,\n",
       "        0.7601966 , 0.77385713, 0.74902065, 0.76203404, 0.77494142,\n",
       "        0.77455204, 0.77585293, 0.77379215, 0.77360675, 0.7766596 ,\n",
       "        0.77373944, 0.77236373, 0.76851268, 0.77897388, 0.76806311,\n",
       "        0.77189951, 0.77161159, 0.76675605, 0.77253264, 0.772203  ,\n",
       "        0.76872844, 0.7755909 , 0.77994367, 0.77233237, 0.76358809,\n",
       "        0.75622425, 0.77113299, 0.76383053, 0.7575255 , 0.77635666,\n",
       "        0.78154301, 0.7815036 , 0.76657837, 0.76661936, 0.75344091,\n",
       "        0.76978604, 0.76617361, 0.77712934, 0.77547619, 0.78490416,\n",
       "        0.78664663, 0.77843178, 0.76773511, 0.75454447, 0.77097817,\n",
       "        0.77444388, 0.77824385, 0.77025118, 0.76649686, 0.7720836 ,\n",
       "        0.77083648, 0.77981629, 0.784991  , 0.76064997, 0.77668936,\n",
       "        0.77269016, 0.75946726, 0.77102685, 0.77292678, 0.76424398,\n",
       "        0.77905144, 0.77385118, 0.76711214, 0.77304519, 0.77073093,\n",
       "        0.75939314, 0.76501029, 0.772474  , 0.76448598, 0.76678318,\n",
       "        0.76170214, 0.76322203, 0.77512183, 0.76506322, 0.76542129,\n",
       "        0.76790543, 0.77570304, 0.77107573, 0.76768174, 0.76625613,\n",
       "        0.77661887, 0.76735559, 0.77294284, 0.76798954, 0.76973471,\n",
       "        0.77564125, 0.76537849, 0.77574227, 0.75612181, 0.77325988,\n",
       "        0.76772935, 0.77657427, 0.77433716, 0.76446701, 0.77327045,\n",
       "        0.75383841, 0.77852636, 0.78427631, 0.76335112, 0.77877861,\n",
       "        0.77193573, 0.75552224, 0.76652951, 0.7717362 , 0.75266831,\n",
       "        0.77053142, 0.78433423, 0.75012768, 0.77879384, 0.77090047,\n",
       "        0.76127148, 0.76638594, 0.76921041, 0.75357093, 0.77524461,\n",
       "        0.77321475, 0.7542997 , 0.77275054, 0.76718081, 0.75130886,\n",
       "        0.76558946, 0.7638373 , 0.74977376, 0.77437986, 0.77055297,\n",
       "        0.75821777, 0.76464576, 0.77025847, 0.76065981, 0.7662215 ,\n",
       "        0.76820619, 0.76007721, 0.76392423, 0.77468966, 0.77219822,\n",
       "        0.77050639, 0.76459124, 0.76331716, 0.77468156, 0.77107353,\n",
       "        0.74657582, 0.77671548, 0.77467361, 0.75554252, 0.76815099,\n",
       "        0.78137473, 0.74802047, 0.7686592 , 0.76877585, 0.74153906,\n",
       "        0.76988676, 0.77731422, 0.73909243, 0.76346634, 0.77491034,\n",
       "        0.73463481, 0.76788138, 0.77431274, 0.73949473, 0.76888996,\n",
       "        0.77823304, 0.74431192, 0.76973079, 0.77923653, 0.73713447,\n",
       "        0.75830805, 0.77162627, 0.75001933, 0.76005964, 0.77784859,\n",
       "        0.74600439, 0.76386209, 0.76411666, 0.74155683, 0.76165529,\n",
       "        0.7756291 , 0.74907294, 0.76902631, 0.78196176, 0.75332289,\n",
       "        0.77279301, 0.77075146, 0.74898488, 0.76454689, 0.77853796,\n",
       "        0.70504803, 0.74940426, 0.74362464, 0.70207529, 0.7508649 ,\n",
       "        0.75290591, 0.70680258, 0.74779252, 0.75667955, 0.71726046,\n",
       "        0.74268574, 0.75235625, 0.71588521, 0.74188068, 0.75434221,\n",
       "        0.6968622 , 0.73794019, 0.75435934, 0.72526153, 0.75005824,\n",
       "        0.7566773 , 0.73561089, 0.74536315, 0.75815664, 0.71887686,\n",
       "        0.74294012, 0.76322834, 0.73176219, 0.74752532, 0.75535717,\n",
       "        0.73026127, 0.74559907, 0.7610052 , 0.71752285, 0.74322064,\n",
       "        0.75985297, 0.7269299 , 0.75390639, 0.76020192, 0.72691292,\n",
       "        0.7510766 , 0.76201766, 0.72215166, 0.74561327, 0.7634803 ]),\n",
       " 'std_test_score': array([0.03273069, 0.04599057, 0.0547373 , 0.04642303, 0.05482315,\n",
       "        0.04388121, 0.05455261, 0.04758839, 0.04336797, 0.04544344,\n",
       "        0.05196629, 0.03863991, 0.04263694, 0.03580744, 0.03887191,\n",
       "        0.04689809, 0.04361717, 0.03641738, 0.0585227 , 0.05019065,\n",
       "        0.05516163, 0.05667653, 0.05294913, 0.03971985, 0.05312597,\n",
       "        0.04641209, 0.03241223, 0.04700957, 0.04488938, 0.0422178 ,\n",
       "        0.0622573 , 0.04326594, 0.03772502, 0.05009109, 0.04129586,\n",
       "        0.03561877, 0.04925861, 0.04377258, 0.03839692, 0.0524212 ,\n",
       "        0.04382682, 0.04032782, 0.05489659, 0.03767369, 0.03345849,\n",
       "        0.03593443, 0.05351025, 0.04430119, 0.03545541, 0.05066606,\n",
       "        0.04956074, 0.0444541 , 0.05045162, 0.04877281, 0.04350458,\n",
       "        0.05124428, 0.05035532, 0.04439227, 0.04623476, 0.04733755,\n",
       "        0.05361763, 0.05445022, 0.04158462, 0.06333664, 0.05529273,\n",
       "        0.05041946, 0.06389614, 0.04411639, 0.04814579, 0.04997856,\n",
       "        0.04767595, 0.04617896, 0.06782458, 0.04910404, 0.04478427,\n",
       "        0.05642483, 0.05084327, 0.04128784, 0.05029965, 0.04306147,\n",
       "        0.03658008, 0.0672276 , 0.04971728, 0.03900358, 0.05508906,\n",
       "        0.04526825, 0.03775959, 0.04945065, 0.04179275, 0.03756962,\n",
       "        0.04128302, 0.04971162, 0.04665737, 0.03999018, 0.05441136,\n",
       "        0.04403269, 0.04459091, 0.04615469, 0.0526542 , 0.03863129,\n",
       "        0.04637101, 0.0517326 , 0.03698051, 0.05241084, 0.0464639 ,\n",
       "        0.0343877 , 0.06068138, 0.05034438, 0.05621873, 0.05152372,\n",
       "        0.0473631 , 0.05620849, 0.05466779, 0.04257872, 0.04081701,\n",
       "        0.05418446, 0.04655117, 0.05709616, 0.05728324, 0.04880323,\n",
       "        0.05902526, 0.0604705 , 0.04688203, 0.04978578, 0.05222449,\n",
       "        0.04104733, 0.06201488, 0.05562067, 0.04858247, 0.06149663,\n",
       "        0.04915566, 0.04466579, 0.04835385, 0.05475766, 0.03733395,\n",
       "        0.039115  , 0.03963317, 0.03907409, 0.05161929, 0.04603162,\n",
       "        0.05157095, 0.04538594, 0.0474633 , 0.05031648, 0.04646517,\n",
       "        0.04665978, 0.048993  , 0.04818755, 0.04564168, 0.04885164,\n",
       "        0.04634543, 0.05071825, 0.06068138, 0.04854137, 0.05750185,\n",
       "        0.05357197, 0.04918693, 0.0569363 , 0.04843405, 0.04866569,\n",
       "        0.05532952, 0.05932907, 0.05420968, 0.05889785, 0.06208822,\n",
       "        0.05749608, 0.06625892, 0.05713068, 0.04293706, 0.05194328,\n",
       "        0.06049999, 0.05274671, 0.05679524, 0.04832773, 0.06218835,\n",
       "        0.06887819, 0.05799551, 0.04993215, 0.05239078, 0.05272984,\n",
       "        0.04707405, 0.0538766 , 0.0482042 , 0.04929275, 0.0562234 ,\n",
       "        0.04492711, 0.04492886, 0.05020161, 0.04618762, 0.0453435 ,\n",
       "        0.05166539, 0.05187702, 0.05384478, 0.05640146, 0.04751109,\n",
       "        0.04256625, 0.05607714, 0.04431303, 0.05064044, 0.05133231,\n",
       "        0.05047416, 0.0533002 , 0.05282524, 0.04607996, 0.0349273 ,\n",
       "        0.05493688, 0.04561167, 0.03668677, 0.04967747, 0.0533113 ,\n",
       "        0.05950718, 0.05310962, 0.0521797 , 0.04953592, 0.05314409,\n",
       "        0.05203417, 0.05493604, 0.05062006, 0.05098347, 0.06019372,\n",
       "        0.05331866, 0.05424989, 0.04567577, 0.05534024, 0.05087692,\n",
       "        0.03273069, 0.04599057, 0.0547373 , 0.04642303, 0.05482315,\n",
       "        0.04388121, 0.05455261, 0.04758839, 0.04336797, 0.04544344,\n",
       "        0.05196629, 0.03863991, 0.04263694, 0.03580744, 0.03887191,\n",
       "        0.04689809, 0.04361717, 0.03641738, 0.0585227 , 0.05019065,\n",
       "        0.05516163, 0.05667653, 0.05294913, 0.03971985, 0.05312597,\n",
       "        0.04641209, 0.03241223, 0.04700957, 0.04488938, 0.0422178 ,\n",
       "        0.0622573 , 0.04326594, 0.03772502, 0.05009109, 0.04129586,\n",
       "        0.03561877, 0.04925861, 0.04377258, 0.03839692, 0.0524212 ,\n",
       "        0.04382682, 0.04032782, 0.05489659, 0.03767369, 0.03345849,\n",
       "        0.03593443, 0.05351025, 0.04430119, 0.03545541, 0.05066606,\n",
       "        0.04956074, 0.0444541 , 0.05045162, 0.04877281, 0.04350458,\n",
       "        0.05124428, 0.05035532, 0.04439227, 0.04623476, 0.04733755,\n",
       "        0.05361763, 0.05445022, 0.04158462, 0.06333664, 0.05529273,\n",
       "        0.05041946, 0.06389614, 0.04411639, 0.04814579, 0.04997856,\n",
       "        0.04767595, 0.04617896, 0.06782458, 0.04910404, 0.04478427,\n",
       "        0.05642483, 0.05084327, 0.04128784, 0.05029965, 0.04306147,\n",
       "        0.03658008, 0.0672276 , 0.04971728, 0.03900358, 0.05508906,\n",
       "        0.04526825, 0.03775959, 0.04945065, 0.04179275, 0.03756962,\n",
       "        0.04128302, 0.04971162, 0.04665737, 0.03999018, 0.05441136,\n",
       "        0.04403269, 0.04459091, 0.04615469, 0.0526542 , 0.03863129,\n",
       "        0.04637101, 0.0517326 , 0.03698051, 0.05241084, 0.0464639 ,\n",
       "        0.0343877 , 0.06068138, 0.05034438, 0.05621873, 0.05152372,\n",
       "        0.0473631 , 0.05620849, 0.05466779, 0.04257872, 0.04081701,\n",
       "        0.05418446, 0.04655117, 0.05709616, 0.05728324, 0.04880323,\n",
       "        0.05902526, 0.0604705 , 0.04688203, 0.04978578, 0.05222449,\n",
       "        0.04104733, 0.06201488, 0.05562067, 0.04858247, 0.06149663,\n",
       "        0.04915566, 0.04466579, 0.04835385, 0.05475766, 0.03733395,\n",
       "        0.039115  , 0.03963317, 0.03907409, 0.05161929, 0.04603162,\n",
       "        0.05157095, 0.04538594, 0.0474633 , 0.05031648, 0.04646517,\n",
       "        0.04665978, 0.048993  , 0.04818755, 0.04564168, 0.04885164,\n",
       "        0.04634543, 0.05071825, 0.06068138, 0.04854137, 0.05750185,\n",
       "        0.05357197, 0.04918693, 0.0569363 , 0.04843405, 0.04866569,\n",
       "        0.05532952, 0.05932907, 0.05420968, 0.05889785, 0.06208822,\n",
       "        0.05749608, 0.06625892, 0.05713068, 0.04293706, 0.05194328,\n",
       "        0.06049999, 0.05274671, 0.05679524, 0.04832773, 0.06218835,\n",
       "        0.06887819, 0.05799551, 0.04993215, 0.05239078, 0.05272984,\n",
       "        0.04707405, 0.0538766 , 0.0482042 , 0.04929275, 0.0562234 ,\n",
       "        0.04492711, 0.04492886, 0.05020161, 0.04618762, 0.0453435 ,\n",
       "        0.05166539, 0.05187702, 0.05384478, 0.05640146, 0.04751109,\n",
       "        0.04256625, 0.05607714, 0.04431303, 0.05064044, 0.05133231,\n",
       "        0.05047416, 0.0533002 , 0.05282524, 0.04607996, 0.0349273 ,\n",
       "        0.05493688, 0.04561167, 0.03668677, 0.04967747, 0.0533113 ,\n",
       "        0.05950718, 0.05310962, 0.0521797 , 0.04953592, 0.05314409,\n",
       "        0.05203417, 0.05493604, 0.05062006, 0.05098347, 0.06019372,\n",
       "        0.05331866, 0.05424989, 0.04567577, 0.05534024, 0.05087692,\n",
       "        0.03539035, 0.05116443, 0.04820586, 0.05362337, 0.0600453 ,\n",
       "        0.05408139, 0.05398968, 0.05063795, 0.04879661, 0.04477185,\n",
       "        0.04697878, 0.04457835, 0.05153355, 0.05454508, 0.05082509,\n",
       "        0.05575448, 0.05927299, 0.0421926 , 0.05435914, 0.04957194,\n",
       "        0.03643849, 0.05715325, 0.0506946 , 0.04409704, 0.05686039,\n",
       "        0.04421478, 0.03187124, 0.05246155, 0.04759593, 0.03862123,\n",
       "        0.06923244, 0.0564495 , 0.04313838, 0.04787171, 0.04791829,\n",
       "        0.03532593, 0.05234509, 0.04335553, 0.04313881, 0.0647045 ,\n",
       "        0.0492686 , 0.04947873, 0.05944671, 0.04878066, 0.03807703,\n",
       "        0.03939238, 0.04000173, 0.04495972, 0.04857093, 0.05015794,\n",
       "        0.05118102, 0.05088318, 0.05909302, 0.05407741, 0.04626874,\n",
       "        0.04676504, 0.05099904, 0.04681398, 0.04539126, 0.04803786,\n",
       "        0.04551337, 0.05933248, 0.05238183, 0.05794512, 0.05050671,\n",
       "        0.06248551, 0.04985415, 0.04163115, 0.04639299, 0.04887407,\n",
       "        0.0524112 , 0.05601322, 0.05849578, 0.03949834, 0.04392981,\n",
       "        0.05890431, 0.04703614, 0.04037017, 0.05113429, 0.05752677,\n",
       "        0.03913168, 0.06923447, 0.04605049, 0.04403761, 0.06400556,\n",
       "        0.05212723, 0.05010053, 0.05979136, 0.05852062, 0.04988251,\n",
       "        0.04365701, 0.03704079, 0.03446061, 0.0567517 , 0.04520231,\n",
       "        0.05220157, 0.05060508, 0.04698921, 0.05335357, 0.04639818,\n",
       "        0.04551531, 0.04419545, 0.0572254 , 0.05113588, 0.04166054,\n",
       "        0.04001545, 0.05593426, 0.05468323, 0.0506997 , 0.04885236,\n",
       "        0.06130245, 0.04596595, 0.0492026 , 0.04747878, 0.04366233,\n",
       "        0.05960979, 0.05068092, 0.05539451, 0.04902497, 0.04879298,\n",
       "        0.05638443, 0.05179689, 0.04542236, 0.04688625, 0.06313016,\n",
       "        0.0545367 , 0.0630284 , 0.04738557, 0.04539552, 0.06174471,\n",
       "        0.05032727, 0.04866165, 0.04899342, 0.0603474 , 0.04496437,\n",
       "        0.05245753, 0.03197261, 0.03976912, 0.05552985, 0.04321543,\n",
       "        0.04943542, 0.04426913, 0.04562528, 0.05071827, 0.05146541,\n",
       "        0.04880906, 0.04782464, 0.05313867, 0.05072887, 0.05040496,\n",
       "        0.03768981, 0.05513288, 0.05522477, 0.05267206, 0.0521186 ,\n",
       "        0.0543099 , 0.05592615, 0.04873345, 0.04945336, 0.03897559,\n",
       "        0.04879844, 0.06098662, 0.05445694, 0.0569764 , 0.05081743,\n",
       "        0.05847043, 0.05359018, 0.06190358, 0.03908591, 0.05460859,\n",
       "        0.05890778, 0.05794964, 0.05371772, 0.04729104, 0.05982197,\n",
       "        0.05158125, 0.05542055, 0.0415503 , 0.05721266, 0.05883313,\n",
       "        0.06276121, 0.05090296, 0.04969962, 0.06273879, 0.05101707,\n",
       "        0.05007935, 0.06405857, 0.04830281, 0.05464566, 0.05782756,\n",
       "        0.05222741, 0.05758243, 0.06195087, 0.0532675 , 0.05589468,\n",
       "        0.0510153 , 0.04034404, 0.04028842, 0.06171576, 0.05739267,\n",
       "        0.05180196, 0.06202219, 0.05544577, 0.05075878, 0.04637382,\n",
       "        0.03976787, 0.04280033, 0.05652069, 0.0489542 , 0.04762808,\n",
       "        0.05413167, 0.0507862 , 0.04827062, 0.04508706, 0.03169567,\n",
       "        0.03323744, 0.05496032, 0.05470154, 0.04831559, 0.05143786,\n",
       "        0.05091324, 0.04678491, 0.0463078 , 0.03729708, 0.04734603]),\n",
       " 'rank_test_score': array([ 57,  18, 182, 323, 143, 105, 247, 229,   1, 265, 273, 479, 120,\n",
       "        350, 363, 457,  76, 132,  32,  92, 516,  30, 400, 441,  13, 156,\n",
       "        204, 288, 292, 553, 217,  36, 267, 145, 130,  82, 194, 447, 566,\n",
       "        174, 210, 372,  10,  74,  98, 431, 356, 163,  67,  88,  34,  80,\n",
       "        161,  65, 309, 122, 474, 369, 202, 518, 439, 139,  72, 374, 103,\n",
       "         84, 390, 334, 261, 126,   5,   3, 354, 134, 379,  63, 299, 111,\n",
       "         78, 170,  70, 295, 113, 418, 191,  49,   7, 149,  94,  53, 545,\n",
       "        179,  44, 427, 128,  42, 468, 184, 200, 493,  59, 151, 573, 297,\n",
       "        227, 555, 280, 239, 337,  47,  26, 549, 423, 504, 320, 277, 219,\n",
       "        339,  86, 237, 352, 397, 154, 425, 305,  40, 249, 212, 165, 116,\n",
       "        366,  51, 302, 159,  28, 536, 251, 100, 527, 360, 221, 569, 395,\n",
       "        514, 564, 207,  38, 562, 307,  55, 531, 487, 280, 433, 312,  16,\n",
       "        543, 176,  24, 420, 270, 387, 329, 141, 253, 465, 137,  96, 501,\n",
       "        258, 453, 318, 385,  21, 470, 186, 197, 415, 147, 315, 634, 610,\n",
       "        490, 640, 606, 463, 659, 594, 575, 648, 586, 376, 638, 613, 358,\n",
       "        656, 608, 460, 579, 483, 235, 582, 382, 286, 654, 551, 346, 571,\n",
       "        499, 332, 444, 557, 412, 603, 508, 403, 451, 540, 476, 455, 534,\n",
       "        241, 623, 520, 214,  57,  18, 182, 323, 143, 105, 247, 229,   1,\n",
       "        265, 273, 479, 120, 350, 363, 457,  76, 132,  32,  92, 516,  30,\n",
       "        400, 441,  13, 156, 204, 288, 292, 553, 217,  36, 267, 145, 130,\n",
       "         82, 194, 447, 566, 174, 210, 372,  10,  74,  98, 431, 356, 163,\n",
       "         67,  88,  34,  80, 161,  65, 309, 122, 474, 369, 202, 518, 439,\n",
       "        139,  72, 374, 103,  84, 390, 334, 261, 126,   5,   3, 354, 134,\n",
       "        379,  63, 299, 111,  78, 170,  70, 295, 113, 418, 191,  49,   7,\n",
       "        149,  94,  53, 545, 179,  44, 427, 128,  42, 468, 184, 200, 493,\n",
       "         59, 151, 573, 297, 227, 555, 280, 239, 337,  47,  26, 549, 423,\n",
       "        504, 320, 277, 219, 339,  86, 237, 352, 397, 154, 425, 305,  40,\n",
       "        249, 212, 165, 116, 366,  51, 302, 159,  28, 536, 251, 100, 527,\n",
       "        360, 221, 569, 395, 514, 564, 207,  38, 562, 307,  55, 531, 487,\n",
       "        280, 433, 312,  16, 543, 176,  24, 420, 270, 387, 329, 141, 253,\n",
       "        465, 137,  96, 501, 258, 453, 318, 385,  21, 470, 186, 197, 415,\n",
       "        147, 315, 634, 610, 490, 640, 606, 463, 659, 594, 575, 648, 586,\n",
       "        376, 638, 613, 358, 656, 608, 460, 579, 483, 235, 582, 382, 286,\n",
       "        654, 551, 346, 571, 499, 332, 444, 557, 412, 603, 508, 403, 451,\n",
       "        540, 476, 455, 534, 241, 623, 520, 214, 226, 181, 244, 246, 169,\n",
       "        245, 284, 394, 108, 405, 304, 317, 430, 276, 290, 392, 196,  90,\n",
       "        285, 503, 581, 322, 498, 568, 178,  61,  62, 436, 435, 599, 365,\n",
       "        450, 158, 199,  15,   9, 119, 409, 590, 328, 231, 124, 349, 438,\n",
       "        294, 336,  91,  12, 530, 168, 275, 547, 327, 264, 489, 107, 243,\n",
       "        422, 260, 342, 548, 473, 279, 485, 429, 523, 513, 209, 472, 462,\n",
       "        407, 189, 325, 411, 446, 172, 414, 263, 406, 368, 190, 467, 188,\n",
       "        584, 256, 410, 173, 233, 486, 255, 597, 118,  23, 510, 110, 301,\n",
       "        588, 437, 311, 602, 344,  20, 617, 109, 331, 525, 443, 378, 598,\n",
       "        206, 257, 593, 272, 417, 612, 459, 497, 620, 232, 343, 560, 478,\n",
       "        348, 529, 449, 399, 538, 495, 223, 291, 345, 481, 511, 224, 326,\n",
       "        629, 167, 225, 585, 402,  69, 626, 393, 389, 647, 362, 153, 651,\n",
       "        507, 216, 661, 408, 234, 650, 384, 125, 636, 371, 102, 653, 559,\n",
       "        314, 619, 539, 136, 630, 496, 492, 646, 524, 193, 622, 381,  46,\n",
       "        600, 269, 341, 625, 482, 115, 673, 621, 637, 674, 616, 601, 672,\n",
       "        627, 577, 670, 644, 605, 671, 645, 592, 675, 652, 591, 666, 618,\n",
       "        578, 658, 633, 561, 668, 643, 512, 662, 628, 589, 663, 632, 526,\n",
       "        669, 642, 542, 664, 596, 533, 665, 615, 522, 667, 631, 506],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881871942204813"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793722\n",
      "F1: 0.680556\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12047f8d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=3, missing=nan, n_estimators=500, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
